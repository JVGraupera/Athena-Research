{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQEA1FVL-BdJ"
      },
      "source": [
        "# ML Experiments\n",
        "We want to try to use machine learning to differentiate between differently rated conversations. Here we begin extracting features from the logfiles and experimenting with algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP9Dac7U-2Bk"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import nltk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk import ConfusionMatrix\n",
        "#from nltk.corpus import opinion_lexicon\n",
        "#from nltk.stem import WordNetLemmatizer\n",
        "#from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.classify import SklearnClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import linear_model\n",
        "#from matplotlib import pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import time\n",
        "import sys\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjEuzymZUFOu",
        "outputId": "f9d72669-5cf2-4aaa-f99e-bf1e19f74183"
      },
      "source": [
        "# nltk downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyqVYiEQ_LfJ",
        "outputId": "36b41b9f-b07f-4612-cb5f-cead12de2fcf"
      },
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kWm9vJb_QcB"
      },
      "source": [
        "# Directories\n",
        "DATA_DIR = '/content/drive/Shareddrives/Alexa Prize 4 (2020 21)/Data/Rating Analysis/Rating-wise grouped conversations/Dataset Partitions'\n",
        "GROUP_FOLDER = '/content/drive/Shareddrives/Alexa Prize 4 (2020 21)/Data/Rating Analysis/Rating-wise grouped conversations'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2g1yS_0-xPD"
      },
      "source": [
        "# Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-I_vlXagBCX"
      },
      "source": [
        "# Open file and return dataframe\n",
        "def open_file(filepath, delimiter='\\t'):\n",
        "  logs = pd.read_csv(filepath, sep=delimiter)\n",
        "  return logs\n",
        "# Save new data to a new tsv file\n",
        "def save_to_file(filepath, dataframe, delimiter='\\t'):\n",
        "  dataframe.to_csv(filepath, sep=delimiter, index=False)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgUmX6nz1P6A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "1e6da8f5-a523-4720-e4a6-24f960d34aaa"
      },
      "source": [
        "trainingFrame = open_file(DATA_DIR + \"/Training Data/rating-1-training-set-profanity-tagged.tsv\")\n",
        "trainingFrame"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>turn_count</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>text</th>\n",
              "      <th>grounding_text</th>\n",
              "      <th>response</th>\n",
              "      <th>chosen_ack_rg</th>\n",
              "      <th>chosen_rg</th>\n",
              "      <th>current_topic</th>\n",
              "      <th>midas</th>\n",
              "      <th>finegrained_labels</th>\n",
              "      <th>finegrained_clf_labels</th>\n",
              "      <th>is_profane</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-02-19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>560597369048bdb615f7d10490e47b44f8e64f310f847a...</td>\n",
              "      <td>let's chat</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hi, this is an Alexa Prize Socialbot. &lt;amazon:...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>INTRODUCTION</td>\n",
              "      <td>introduction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['unk']</td>\n",
              "      <td>['more-information']</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-02-19</td>\n",
              "      <td>1.0</td>\n",
              "      <td>560597369048bdb615f7d10490e47b44f8e64f310f847a...</td>\n",
              "      <td>good horrible</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;say-as interpret-as=\"interjection\"&gt;Oh boy.&lt;/s...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>INTRODUCTION</td>\n",
              "      <td>introduction</td>\n",
              "      <td>['comment', 'back-channeling', 'opinion']</td>\n",
              "      <td>['unk', 'unk', 'unk']</td>\n",
              "      <td>['statement-non-opinion', 'statement-non-opini...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-02-19</td>\n",
              "      <td>2.0</td>\n",
              "      <td>560597369048bdb615f7d10490e47b44f8e64f310f847a...</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I get it. I kept it lowkey this year. Next yea...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>INTRODUCTION</td>\n",
              "      <td>introduction</td>\n",
              "      <td>['neg_answer']</td>\n",
              "      <td>['no-answer']</td>\n",
              "      <td>['no-answer']</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-02-19</td>\n",
              "      <td>3.0</td>\n",
              "      <td>560597369048bdb615f7d10490e47b44f8e64f310f847a...</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Well, I hope you are able to stay safe, even i...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>INTRODUCTION</td>\n",
              "      <td>introduction</td>\n",
              "      <td>['neg_answer']</td>\n",
              "      <td>['no-answer']</td>\n",
              "      <td>['no-answer']</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-02-19</td>\n",
              "      <td>4.0</td>\n",
              "      <td>560597369048bdb615f7d10490e47b44f8e64f310f847a...</td>\n",
              "      <td>nothing</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Oh, that's ok! For me, I really want to experi...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>INTRODUCTION</td>\n",
              "      <td>introduction</td>\n",
              "      <td>['other_answers']</td>\n",
              "      <td>['no-answer']</td>\n",
              "      <td>['statement-non-opinion']</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45812</th>\n",
              "      <td>2021-05-06</td>\n",
              "      <td>25.0</td>\n",
              "      <td>d59a141d1f85a01e40827e5ab65791db5c0dbf21493202...</td>\n",
              "      <td>yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;amazon:emotion name=\"excited\" intensity=\"low\"...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RG_SYSTEM_INITIATIVE_MENU</td>\n",
              "      <td>neutral</td>\n",
              "      <td>['pos_answer']</td>\n",
              "      <td>['yes-answer']</td>\n",
              "      <td>['yes-answer']</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45813</th>\n",
              "      <td>2021-05-06</td>\n",
              "      <td>26.0</td>\n",
              "      <td>d59a141d1f85a01e40827e5ab65791db5c0dbf21493202...</td>\n",
              "      <td>good night</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I loved chatting with you, let's talk again so...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GOODBYE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['closing']</td>\n",
              "      <td>['conversation-closing']</td>\n",
              "      <td>['conversation-closing']</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45814</th>\n",
              "      <td>2021-05-06</td>\n",
              "      <td>27.0</td>\n",
              "      <td>d59a141d1f85a01e40827e5ab65791db5c0dbf21493202...</td>\n",
              "      <td>who is trying to conversation</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;amazon:emotion name=\"excited\" intensity=\"low\"...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>DM_GENERATOR</td>\n",
              "      <td>nature</td>\n",
              "      <td>['open_question_factual']</td>\n",
              "      <td>['unk']</td>\n",
              "      <td>['fact-question']</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45815</th>\n",
              "      <td>2021-05-06</td>\n",
              "      <td>28.0</td>\n",
              "      <td>d59a141d1f85a01e40827e5ab65791db5c0dbf21493202...</td>\n",
              "      <td>are you doing</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What do you think about this. &lt;prosody rate=\"9...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NATURE</td>\n",
              "      <td>nature</td>\n",
              "      <td>['abandon']</td>\n",
              "      <td>['conversation-opening']</td>\n",
              "      <td>['personal-question']</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45816</th>\n",
              "      <td>2021-05-06</td>\n",
              "      <td>NaN</td>\n",
              "      <td>d59a141d1f85a01e40827e5ab65791db5c0dbf21493202...</td>\n",
              "      <td>stop alexa stop</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45817 rows Ã— 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             date  ...  is_profane\n",
              "0      2021-02-19  ...           0\n",
              "1      2021-02-19  ...           0\n",
              "2      2021-02-19  ...           0\n",
              "3      2021-02-19  ...           0\n",
              "4      2021-02-19  ...           0\n",
              "...           ...  ...         ...\n",
              "45812  2021-05-06  ...           0\n",
              "45813  2021-05-06  ...           0\n",
              "45814  2021-05-06  ...           0\n",
              "45815  2021-05-06  ...           0\n",
              "45816  2021-05-06  ...           0\n",
              "\n",
              "[45817 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9R0ZgHINoT_"
      },
      "source": [
        "### RG distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uMKvVbkNqNc",
        "outputId": "06ea4512-23ff-41e9-a38a-6d4c69f56796"
      },
      "source": [
        "rg_dist = nltk.FreqDist(trainingFrame['chosen_rg'])\n",
        "rg_dist.most_common(1000)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('INTRODUCTION', 20571),\n",
              " ('CENTER', 2849),\n",
              " (nan, 2622),\n",
              " ('DM_GENERATOR', 1606),\n",
              " ('NATURE', 1303),\n",
              " ('VIDEO_GAMES', 1228),\n",
              " ('MOVIESKG', 1141),\n",
              " ('ANIMALS', 1140),\n",
              " ('TEMPLATE_HANDLER_FALLBACK', 1041),\n",
              " ('MUSICKG', 944),\n",
              " ('redquestion', 881),\n",
              " ('BOOKS', 747),\n",
              " ('MUSIC', 742),\n",
              " ('GOODBYE', 723),\n",
              " ('MOVIES', 688),\n",
              " ('EVI', 686),\n",
              " ('FOOD', 648),\n",
              " ('TVKG', 603),\n",
              " ('SPORTSKG', 569),\n",
              " ('HOBBIES', 526),\n",
              " ('NUTRITION', 504),\n",
              " ('RepeatGenerator', 442),\n",
              " ('DINOSAURS', 434),\n",
              " ('BOARD_GAMES', 433),\n",
              " ('HARRYPOTTER', 408),\n",
              " ('RG_SYSTEM_INITIATIVE_MENU', 383),\n",
              " ('SB_INDEX', 370),\n",
              " ('ASTRONOMY', 293),\n",
              " ('FunctionalMonolith', 263),\n",
              " ('MCU', 257),\n",
              " ('COMIC_BOOKS', 182),\n",
              " ('PIRATES', 180),\n",
              " ('FallbackStrategyInitiative', 109),\n",
              " ('ONTOLOGYBOT', 91),\n",
              " ('IMMenuChances_normal', 76),\n",
              " ('SUPER_BOWL', 48),\n",
              " ('ES_INDEX', 41),\n",
              " ('WAPO', 37),\n",
              " ('WIKIPEDIA', 7),\n",
              " ('CONTROLLED_POLICY_DRIVEN_NRG', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qT0TCAYP0_J"
      },
      "source": [
        "### Topic distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHAa5ZZPvVh",
        "outputId": "798ab3f9-e1b6-42b4-9730-8708489ef460"
      },
      "source": [
        "topic_dist = nltk.FreqDist(trainingFrame['current_topic'])\n",
        "topic_dist.most_common(50)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('introduction', 21789),\n",
              " (nan, 3379),\n",
              " ('movies', 2474),\n",
              " ('music', 2178),\n",
              " ('animals', 1921),\n",
              " ('nature', 1749),\n",
              " ('video_games', 1440),\n",
              " ('sports', 1314),\n",
              " ('books', 1220),\n",
              " ('dinosaurs', 1029),\n",
              " ('harry_potter', 880),\n",
              " ('food', 824),\n",
              " ('hobbies', 793),\n",
              " ('nutrition', 792),\n",
              " ('astronomy', 764),\n",
              " ('tv', 742),\n",
              " ('comic_books', 684),\n",
              " ('board_games', 597),\n",
              " ('pirates', 472),\n",
              " ('menu_topic', 435),\n",
              " ('neutral', 142),\n",
              " ('super_bowl', 68),\n",
              " ('conspiracy', 49),\n",
              " ('news', 46),\n",
              " ('politics', 18),\n",
              " ('history', 7),\n",
              " ('artificial_intelligence', 4),\n",
              " ('donald_trump', 4),\n",
              " ('controversial', 2),\n",
              " ('science_and_technology', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ierH9kj5ZgDT"
      },
      "source": [
        "# LIWC Implementation\n",
        "LIWC code from CSE 143"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d-R_0FvZxFu"
      },
      "source": [
        "#This tries to be similar to LIWC, even LIWC's more questionable decisions....\n",
        "# Examples: dot after a number (123.) is included in the number, hyphens break words, \"Mr.\" is a sentence, yeah...\n",
        "# That said, I refuse to call \"123).\" two words... some things are just too crazy..\n",
        "#The primary interface is score_text(), \n",
        "#  though you may want to use Dictionary's score_word() directly on occasion.\n",
        "#This was built for LIWC's 2007 dictionary with category names/numbers in the dic file\n",
        "#A line with \"%\" should precede the category list, if not, expect crashing\n",
        "#Only words starting with alphanumeric characters count towards the \"Word Count\" \n",
        "#  and are used in the normalizing denominator. \n",
        "#Parenthesis are counted individually, not in groups of two \n",
        "#Please be careful when editing the code, it is more complicated than would be ideal\n",
        "\n",
        "from __future__ import division\n",
        "import re\n",
        "import string\n",
        "import os, sys\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "#This actually captures most of what LIWC does...., I have no idea why it sells...\n",
        "_liwc_tokenizer = re.compile(r'(\\d[\\d\\.\\,\\-\\:]*\\d\\.?|[a-zA-Z][a-zA-Z\\.\\']*[a-zA-Z]|\\S|\\n)',re.UNICODE|re.IGNORECASE) \n",
        "def score_text(text, raw_counts=False, scores=None, unique_words=None):\n",
        "    \"\"\"Returns a sparse counter object of word frequencies or counts if raw_counts is specified\n",
        "        @param scores: If you want to keep a running total, Scores should be \n",
        "            a Counter of previous counts and raw_counts should be set to True!\n",
        "        @param unique_words: Again, will be created if None. Should be a set().\n",
        "            If used, you'll probably want to override the scores['Unique Words'] category.\n",
        "    \"\"\"\n",
        "    if scores == None: scores = Counter()\n",
        "    if unique_words == None: unique_words = set()\n",
        "    \n",
        "    all_tokens = _liwc_tokenizer.findall(text.lower())\n",
        "    sentence_terminated = True\n",
        "    for i in range(len(all_tokens)):\n",
        "        token = all_tokens[i]\n",
        "        if len(token)==0: continue\n",
        "        \n",
        "        if token[0].isdigit(): #Numbers\n",
        "            scores.update(_dictionary.score_word(token))\n",
        "            sentence_terminated=False\n",
        "        elif token[0].isalpha(): #Words\n",
        "            unique_words.add(token)\n",
        "            previous_token = all_tokens[i-1] if i>0 else ''\n",
        "            next_token = all_tokens[i+1] if i<len(all_tokens)-1 else ''\n",
        "            scores.update(_dictionary.score_word(token, previous_token, next_token))\n",
        "            sentence_terminated=False\n",
        "        else: #Punctuation and stuff\n",
        "            scores.update(_dictionary.score_word(token))\n",
        "\n",
        "        if token in Dictionary.sentence_punctuation and not sentence_terminated:\n",
        "            scores['Sentences']+=1\n",
        "            sentence_terminated = True\n",
        "\n",
        "    if not sentence_terminated:\n",
        "        scores['Sentences']+=1\n",
        "    \n",
        "    scores['Unique Words']=len(unique_words)\n",
        "    if scores['Sentences'] > 0:\n",
        "    \tscores['Words Per Sentence']=scores['Word Count']/scores['Sentences']\n",
        "    else:\n",
        "    \tscores['Words Per Sentence'] = 1;\n",
        "    \n",
        "    if not raw_counts:\n",
        "        scores = normalize_scores(scores)\n",
        "    \n",
        "    return scores\n",
        "\n",
        "def score_file(filename, raw_counts=False, scores=None, unique_words=None):\n",
        "    return score_text(open(filename).read(), raw_counts=raw_counts, scores=scores, unique_words=unique_words)\n",
        "\n",
        "def normalize_scores(scores, bound_scores=True):\n",
        "    \"\"\"@summary: Converts counts to percentages\"\"\"\n",
        "    new_scores = Counter()\n",
        "    for category, score in scores.items():\n",
        "        if category not in {'Word Count', 'Sentences','Words Per Sentence', 'Newlines'}:\n",
        "            if scores['Word Count'] > 0:\n",
        "                score = 100.0*score/scores['Word Count']\n",
        "            elif score > 0:\n",
        "                score = 100.0\n",
        "            else:\n",
        "                score = 0.0\n",
        "            if bound_scores: #Since certain categories can exceed word count\n",
        "                score = min(100.0, max(0.0, score)) #Bounds it to [0,100]\n",
        "        new_scores[category]=score\n",
        "    return new_scores\n",
        "\n",
        "class Dictionary():\n",
        "    sentence_punctuation = {'.','?','!','\\n'}\n",
        "    _TYPE_BASIC = 'basic'\n",
        "    _TYPE_PRE = 'pre'\n",
        "    _TYPE_POST = 'post'\n",
        "    \n",
        "    def __init__(self, filename, use_long_category_names=True, internal_category_list=None):\n",
        "        \"\"\"@param internal_category_list: Should be None or '2001' or '2007' \"\"\"\n",
        "        self._stems = dict()#this is a prefix tree for the stems, the leaves are sets of categories\n",
        "        self._lookup = defaultdict(dict) #word->type->????->{categories} \n",
        "                                                    #type can be one of \"basic\", \"pre\", \"post\". \n",
        "                                                    #basic leads to a set of categories, \n",
        "                                                    #pre and post lead to a list of tuples of (conditions, if_true categories, if_false categories)\n",
        "        self._ignored=set() #caches words that are searched for but not found, this favors processing over memory\n",
        "        \n",
        "        self._setup_category_lookup(internal_category_list, use_long_category_names)\n",
        "        try:\n",
        "            self.load_dictionary_file(filename, internal_category_list)\n",
        "        except:\n",
        "            sys.stderr.writelines([\"Failed to load dictionary file: \"+filename+\"\\n\",\n",
        "                                   \"Is the dictionary file correct?\\n\",\n",
        "                                   \"Does a % precede the category list?\\n\",\n",
        "                                   \"If there is no category list, did you set internal_category_list='2007' ?\\n\",\n",
        "                                   \"Hope this helps...\\n\"])\n",
        "            raise\n",
        "    \n",
        "    \n",
        "    _dictionary_line_re =  re.compile(r'^(\\w+)(\\*?)\\s*(.*)$')\n",
        "    _dictionary_line_categories_re = re.compile(r'(\\d+|\\<(\\w+(\\s+\\w+)*)\\>(\\d+)(\\/(\\d+))?|\\(\\s*(\\d+(\\s+\\d+)*)\\s*\\)(\\d+)(\\/(\\d+))?)')\n",
        "    def load_dictionary_file(self, filename, internal_category_list=None):\n",
        "        category_mode = False\n",
        "        for line in open(filename):\n",
        "            line = line.strip()\n",
        "            \n",
        "            if line=='' or line.startswith('#'): \n",
        "                continue\n",
        "            if line.startswith('%'):\n",
        "                category_mode = not category_mode\n",
        "                continue\n",
        "            \n",
        "            if category_mode:\n",
        "                if internal_category_list == None:\n",
        "                    number, category_name = line.split()\n",
        "                    category_name = self._translate_category_name(category_name)\n",
        "                    self._category_lookup[int(number)]=category_name\n",
        "                continue\n",
        "            \n",
        "            word, is_stem, all_word_categories = Dictionary._dictionary_line_re.match(line).groups()\n",
        "            for category_group in Dictionary._dictionary_line_categories_re.findall(all_word_categories):\n",
        "                category = category_group[0]\n",
        "                if category == '00':\n",
        "                    continue\n",
        "                elif category.isdigit():\n",
        "                    if is_stem=='*':\n",
        "                        self._add_stemmed(word, self._category_lookup[int(category)])\n",
        "                    else:\n",
        "                        if Dictionary._TYPE_BASIC not in self._lookup[word]:\n",
        "                            self._lookup[word][Dictionary._TYPE_BASIC]=set()\n",
        "                        self._lookup[word][Dictionary._TYPE_BASIC].add(self._category_lookup[int(category)])\n",
        "                \n",
        "                elif '(' in category or '<' in category: #convoluted special cases lead to much of the complexity in this program\n",
        "                    junk, post, junk, if_post, junk, if_not_post, pre, junk, if_pre, junk, if_not_pre = category_group\n",
        "                    if pre != '':\n",
        "                        entry_type = Dictionary._TYPE_PRE\n",
        "                        conditions = sorted([self._category_lookup[int(number)] for number in pre.split()])\n",
        "                        if_true = self._category_lookup[int(if_pre)]\n",
        "                        if if_not_pre != '':\n",
        "                            if_not_true = self._category_lookup[int(if_not_pre)]\n",
        "                    elif post != '':\n",
        "                        entry_type = Dictionary._TYPE_POST\n",
        "                        conditions = sorted(post.lower().split())\n",
        "                        if_true = self._category_lookup[int(if_post)]\n",
        "                        if if_not_post != '':\n",
        "                            if_not_true = self._category_lookup[int(if_not_post)]\n",
        "                        \n",
        "                    if entry_type not in self._lookup[word]:\n",
        "                            self._lookup[word][entry_type]=list()\n",
        "                    \n",
        "                    for other_conditions, other_if_set, other_if_not_set in self._lookup[word][entry_type]:\n",
        "                        if str(other_conditions)==str(conditions): #a little costly on load means less on use\n",
        "                            other_if_set.add(if_true)\n",
        "                            other_if_not_set.add(if_not_true)\n",
        "                            break\n",
        "                    else: #for else means the for ended naturally\n",
        "                        self._lookup[word][entry_type].append( (conditions, {if_true}, {if_not_true}) )\n",
        "    \n",
        "    def _translate_category_name(self, category_name):\n",
        "        if category_name.lower() in self._category_name_lookup:\n",
        "            return self._category_name_lookup[category_name.lower()]\n",
        "        return category_name\n",
        "    \n",
        "    def _add_stemmed(self, word, category):\n",
        "        current_node = self._stems\n",
        "        for char in word[:-1]:\n",
        "            if char not in current_node:\n",
        "                current_node[char]=dict()\n",
        "            current_node = current_node[char]\n",
        "        if word[-1] not in current_node:\n",
        "            current_node[word[-1]]=set()\n",
        "        current_node = current_node[word[-1]]\n",
        "\n",
        "        current_node.add(category)\n",
        "    \n",
        "    _pure_punctuation_re = re.compile('^['+re.escape(string.punctuation)+']+$')\n",
        "    _punctuation_of_interest = {'?':'Question Marks', '!':'Exclamation Marks', '\"':'Quote Marks',\n",
        "                                ',':'Comma',':':'Colon',';':'Semicolon','-':'Dash','\\'':'Apostrophe',\n",
        "                                '(':'Parenthesis', ')':'Parenthesis', '{':'Parenthesis', '}':'Parenthesis', '[':'Parenthesis', ']':'Parenthesis' }\n",
        "    def score_word(self, word, previous_word=None, next_word=None):\n",
        "        scores = Counter()\n",
        "        if word is None:\n",
        "            return scores\n",
        "        \n",
        "        if '\\n' in word:\n",
        "            scores['Newlines']+=1\n",
        "            \n",
        "        word = word.strip().lower()\n",
        "        \n",
        "        if len(word)==0:\n",
        "            pass\n",
        "        elif word[0].isdigit():\n",
        "            scores['Word Count']+=1\n",
        "            scores['Numerals']+=1\n",
        "        elif Dictionary._pure_punctuation_re.match(word):\n",
        "            scores['All Punctuation']+=1\n",
        "            for char in word:\n",
        "                if char in Dictionary._punctuation_of_interest:\n",
        "                    scores[Dictionary._punctuation_of_interest[char]]+=1\n",
        "                else:\n",
        "                    scores['Other Punctuation']+=1\n",
        "        else:\n",
        "            scores['Word Count']+=1\n",
        "            if len(word) > 6:\n",
        "                scores['Six Letter Words'] += 1\n",
        "            if word not in self._ignored:\n",
        "                if word in self._lookup:\n",
        "                    for entry_type in self._lookup[word]:\n",
        "                        if entry_type==Dictionary._TYPE_BASIC:\n",
        "                            scores.update(self._lookup[word][entry_type])\n",
        "                        else:\n",
        "                            for conditions, if_set, if_not_set in self._lookup[word][entry_type]:\n",
        "                                if ((entry_type==Dictionary._TYPE_PRE and not set(self.score_word(word=previous_word, next_word=word).keys()).isdisjoint(set(conditions))) or \n",
        "                                    (entry_type==Dictionary._TYPE_POST and next_word is not None and next_word.lower() in conditions)):\n",
        "                                    scores.update(if_set)\n",
        "                                else:\n",
        "                                    scores.update(if_not_set)\n",
        "                else:\n",
        "                    current_node = self._stems\n",
        "                    for char in word:\n",
        "                        if char in current_node:\n",
        "                            current_node = current_node[char]\n",
        "                            if isinstance(current_node, set):\n",
        "                                if Dictionary._TYPE_BASIC not in self._lookup[word]:\n",
        "                                    self._lookup[word][Dictionary._TYPE_BASIC]=set()\n",
        "                                self._lookup[word][Dictionary._TYPE_BASIC].update(current_node) #add to main lookup for time efficiency\n",
        "                                scores.update(self._lookup[word][Dictionary._TYPE_BASIC])\n",
        "                                break\n",
        "                        else:\n",
        "                            self._ignored.add(word) #dead end\n",
        "                            break\n",
        "                    else:\n",
        "                        self._ignored.add(word) #not found but didn't hit a dead end\n",
        "\n",
        "                if word not in self._ignored: #Note this is \"still not in\"\n",
        "                    scores['Dictionary Words']+=1\n",
        "        return scores\n",
        "    \n",
        "    def _setup_category_lookup(self, internal_category_list, use_long_category_names):\n",
        "        self._category_name_lookup = dict()\n",
        "        if use_long_category_names:\n",
        "            for long_name, LIWC2007_number, LIWC2007_short, LIWC2001_number, LIWC2001_short in Dictionary._liwc_categories:\n",
        "                if LIWC2001_short is not None:\n",
        "                    self._category_name_lookup[LIWC2001_short]=long_name\n",
        "                if LIWC2007_short is not None:\n",
        "                    self._category_name_lookup[LIWC2007_short]=long_name\n",
        "        \n",
        "        self._category_lookup = dict()\n",
        "        if internal_category_list is not None:\n",
        "            for long_name, LIWC2007_number, LIWC2007_short, LIWC2001_number, LIWC2001_short in Dictionary._liwc_categories:\n",
        "                if internal_category_list == '2001' and LIWC2001_number is not None:\n",
        "                    self._category_lookup[LIWC2001_number]=self._translate_category_name(LIWC2001_short)\n",
        "                if internal_category_list == '2007' and LIWC2007_number is not None:\n",
        "                    self._category_lookup[LIWC2007_number]=self._translate_category_name(LIWC2007_short)\n",
        "        \n",
        "    #In case it is needed:\n",
        "    #(long_name, LIWC2007_number, LIWC2007_short, LIWC2001_number, LIWC2001_short)\n",
        "    _liwc_categories =  [\n",
        "    ('Total Function Words',1,'funct',None,None), \n",
        "    ('Total Pronouns',2,'pronoun',1,'pronoun'), \n",
        "    ('Personal Pronouns',3,'ppron',None,None), \n",
        "    ('First Person Singular',4,'i',2,'i'), \n",
        "    ('First Person Plural',5,'we',3,'we'), \n",
        "    ('Second Person',6,'you',5,'you'), \n",
        "    ('Third Person Singular',7,'shehe',None,None), \n",
        "    ('Third Person Plural',8,'they',None,None), \n",
        "    (' Impersonal Pronouns',9,'ipron',None,None), \n",
        "    ('Articles',10,'article',9,'article'), \n",
        "    ('Common Verbs',11,'verb',None,None), \n",
        "    ('Auxiliary Verbs',12,'auxverb',None,None), \n",
        "    ('Past Tense',13,'past',38,'past'), \n",
        "    ('Present Tense',14,'present',39,'present'), \n",
        "    ('Future Tense',15,'future',40,'future'), \n",
        "    ('Adverbs',16,'adverb',None,None), \n",
        "    ('Prepositions',17,'preps',10,'preps'), \n",
        "    ('Conjunctions',18,'conj',None,None), \n",
        "    ('Negations',19,'negate',7,'negate'), \n",
        "    ('Quantifiers',20,'quant',None,None), \n",
        "    ('Number',21,'number',11,'number'), \n",
        "    ('Swear Words',22,'swear',66,'swear'), \n",
        "    ('Social Processes',121,'social',31,'social'), \n",
        "    ('Family',122,'family',35,'family'), \n",
        "    ('Friends',123,'friend',34,'friends'), \n",
        "    ('Humans',124,'humans',36,'humans'), \n",
        "    ('Affective Processes',125,'affect',12,'affect'), \n",
        "    ('Positive Emotion',126,'posemo',13,'posemo'), \n",
        "    ('Negative Emotion',127,'negemo',16,'negemo'), \n",
        "    ('Anxiety',128,'anx',17,'anx'), \n",
        "    ('Anger',129,'anger',18,'anger'), \n",
        "    ('Sadness',130,'sad',19,'sad'), \n",
        "    ('Cognitive Processes',131,'cogmech',20,'cogmech'), \n",
        "    ('Insight',132,'insight',22,'insight'), \n",
        "    ('Causation',133,'cause',21,'cause'), \n",
        "    ('Discrepancy',134,'discrep',23,'discrep'), \n",
        "    ('Tentative',135,'tentat',25,'tentat'), \n",
        "    ('Certainty',136,'certain',26,'certain'), \n",
        "    ('Inhibition',137,'inhib',24,'inhib'), \n",
        "    ('Inclusive',138,'incl',44,'incl'), \n",
        "    ('Exclusive',139,'excl',45,'excl'), \n",
        "    ('Perceptual Processes',140,'percept',27,'senses'), \n",
        "    ('See',141,'see',28,'see'), \n",
        "    ('Hear',142,'hear',29,'hear'), \n",
        "    ('Feel',143,'feel',30,'feel'), \n",
        "    ('Biological Processes',146,'bio',None,None), \n",
        "    ('Body',147,'body',61,'body'), \n",
        "    ('Health',148,'health',None,None), \n",
        "    ('Sexual',149,'sexual',62,'sexual'), \n",
        "    ('Ingestion',150,'ingest',63,'eating'), \n",
        "    ('Relativity',250,'relativ',None,None), \n",
        "    ('Motion',251,'motion',46,'motion'), \n",
        "    ('Space',252,'space',41,'space'), \n",
        "    ('Time',253,'time',37,'time'), \n",
        "    ('Work',354,'work',49,'job'), \n",
        "    ('Achievement',355,'achieve',50,'achieve'), \n",
        "    ('Leisure',356,'leisure',51,'leisure'), \n",
        "    ('Home',357,'home',52,'home'), \n",
        "    ('Money',358,'money',56,'money'), \n",
        "    ('Religion',359,'relig',58,'relig'), \n",
        "    ('Death',360,'death',59,'death'), \n",
        "    ('Assent',462,'assent',8,'assent'), \n",
        "    ('Nonfluencies',463,'nonfl',67,'nonfl'), \n",
        "    ('Fillers',464,'filler',68,'fillers'), \n",
        "    ('Total first person',None,None,4,'self'), \n",
        "    ('Total third person',None,None,6,'other'), \n",
        "    ('Positive feelings',None,None,14,'posfeel'), \n",
        "    ('Optimism and energy',None,None,15,'optim'), \n",
        "    ('Communication',None,None,32,'comm'), \n",
        "    ('Other references to people',None,None,33,'othref'), \n",
        "    ('Up',None,None,42,'up'), \n",
        "    ('Down',None,None,43,'down'), \n",
        "    ('Occupation',None,None,47,'occup'), \n",
        "    ('School',None,None,48,'school'), \n",
        "    ('Sports',None,None,53,'sports'), \n",
        "    ('TV',None,None,54,'tv'), \n",
        "    ('Music',None,None,55,'music'), \n",
        "    ('Metaphysical issues',None,None,57,'metaph'), \n",
        "    ('Physical states and functions',None,None,60,'physcal'), \n",
        "    ('Sleeping',None,None,64,'sleep'), \n",
        "    ('Grooming',None,None,65,'groom')]\n",
        "\n",
        "# _dictionary_filename = os.path.abspath(os.path.join(\"./\", 'data'))+'/LIWC2007.dic'\n",
        "_dictionary_filename = os.path.join(GROUP_FOLDER, \"LIWC\", \"LIWC2007.dic\")\n",
        "_dictionary=Dictionary(_dictionary_filename)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx0BWWrqaafi",
        "outputId": "17eeee66-5a12-41f2-d0f5-eee128ba5307"
      },
      "source": [
        "# Testing LIWC\n",
        "txt = \"Does this work? I don't know.\"\n",
        "print(txt)\n",
        "print(score_text(txt))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Does this work? I don't know.\n",
            "Counter({'Unique Words': 100.0, 'Dictionary Words': 33.333333333333336, 'All Punctuation': 33.333333333333336, 'Achievement': 16.666666666666668, 'Work': 16.666666666666668, 'Question Marks': 16.666666666666668, 'Insight': 16.666666666666668, 'Common Verbs': 16.666666666666668, 'Cognitive Processes': 16.666666666666668, 'Present Tense': 16.666666666666668, 'Other Punctuation': 16.666666666666668, 'Word Count': 6, 'Words Per Sentence': 3.0, 'Sentences': 2})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbgnJNiFCWry"
      },
      "source": [
        "# Compile Data About Conversations\n",
        "Here, I think it would be useful to construct new dataframes which contain useful information about each conversation (like the user text, system text, length of the conversation, etc...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PeJiGMEConw",
        "outputId": "b95f7f79-63e0-45f9-cbba-72d6db5fc21f"
      },
      "source": [
        "conversation_ids = list(set(trainingFrame['conversation_id']))\n",
        "print(\"There are {} conversations in this dataframe.\".format(len(conversation_ids)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 2926 conversations in this dataframe.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMoc1hA-R_Sh"
      },
      "source": [
        "# Regex extraction functions\n",
        "def extract_system_text(system_utter):\n",
        "  # Extract normal text from the system utterance and return it\n",
        "  # Find all interjections of the form <string>\n",
        "  text_search = re.finditer(r'\\<[^<>]*\\>', system_utter)\n",
        "  new_text = \"\"\n",
        "  index = 0\n",
        "  # Step through the matches, taking only the text outside the spans\n",
        "  for match in text_search:\n",
        "    new_text += system_utter[index:match.span()[0]]\n",
        "    index = match.span()[1]\n",
        "  # Add the last bit of the utterance\n",
        "  # (or the entire thing if no matches were found)\n",
        "  new_text += system_utter[index:]\n",
        "  return new_text\n",
        "\n",
        "def extract_midas_labels(midas_string):\n",
        "  # Convert the midas string into a list of strings\n",
        "  # Matches either all word characters enclosed in ' ' or in \" \", depending on which is present\n",
        "  return [match1 if re.search(r\"\\w\", match1) else match2 for match1, match2 in re.findall(r\"\\\"([^\\\"]*)\\\"|'([^']*)'\", midas_string)]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIE09rVU7nC"
      },
      "source": [
        "tests = [\"['hello', 'there']\", \"['hello', \\\"they're\\\"]\", \"[\\\"how's\\\", \\\"they're\\\"]\", \"[\\\"www'''w'w'w'w'\\\", \\\"asdf''asdfsadf'''asdf\\\"]\"]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCeVdoLqU-eh",
        "outputId": "da4ea4bb-a2d1-46d9-bd57-b444c2b74265"
      },
      "source": [
        "for test in tests:\n",
        "  print(extract_midas_labels(test))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'there']\n",
            "['hello', \"they're\"]\n",
            "[\"how's\", \"they're\"]\n",
            "[\"www'''w'w'w'w'\", \"asdf''asdfsadf'''asdf\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d__Q0ktIBjNB"
      },
      "source": [
        "# Extract and compile info about each conversation\n",
        "def compile_info(dataframe, rating, min_length):\n",
        "  \"\"\"\n",
        "  Iterate through the dataframe and compile information about each conversation.\n",
        "  This information includes:\n",
        "    The conversation ID \n",
        "    The length of the conversation\n",
        "    The date at which the conversation took place\n",
        "    The user utterances\n",
        "    The system utterances\n",
        "    A distribution of the topics that appeared\n",
        "    A distribution of the response generators that were used by the system\n",
        "    The rating (for later classification tasks)\n",
        "    Number of profane tagged user utterences\n",
        "    A list of midas tag lists for each conversation\n",
        "  \n",
        "  Params:\n",
        "    dataframe : pandas DataFrame of a logfile\n",
        "    rating : int, the rating of the conversations which appear in the dataframe\n",
        "    min_length : int, the minimum number of turns allowed\n",
        "  Returns:\n",
        "    A new pandas DataFrame containing the above information. Each row is one conversation.\n",
        "  \"\"\"\n",
        "  # compiled_info is a list of rows, each in the form of a list\n",
        "  compiled_info = []\n",
        "  # Iterate through the dataframe, extracting data about conversations\n",
        "  # and adding it to the compiled_info list\n",
        "  current_conv_id = dataframe['conversation_id'][0]\n",
        "  # Objects to hold information about the current conversation\n",
        "  user_text = []\n",
        "  system_text = []\n",
        "  conv_topic_dist = nltk.FreqDist()\n",
        "  conv_rg_dist = nltk.FreqDist()\n",
        "  conv_len = 0\n",
        "  profanities = 0\n",
        "  midas = []\n",
        "  for i in range(len(dataframe)):\n",
        "    # If the conversation id changes, we have a new conversation beginning\n",
        "    if not dataframe['conversation_id'][i] == current_conv_id:\n",
        "      # If conversation is long enough,\n",
        "      # Add info to compiled_info list\n",
        "      if conv_len >= min_length:\n",
        "        compiled_info.append([current_conv_id, rating, dataframe['date'][i-1], conv_len, user_text, system_text, conv_topic_dist, conv_rg_dist, profanities, midas])\n",
        "      # Reset the conversation\n",
        "      user_text = []\n",
        "      system_text = []\n",
        "      midas = []\n",
        "      conv_len = 0\n",
        "      conv_topic_dist = nltk.FreqDist()\n",
        "      conv_rg_dist = nltk.FreqDist()\n",
        "      profanities = 0\n",
        "      # Update current conversation id\n",
        "      current_conv_id = dataframe['conversation_id'][i]\n",
        "    # Compile info for each row in a conversation\n",
        "    # If we have a non-terminal row, add one to the conversation length\n",
        "    if not np.isnan(dataframe['turn_count'][i]):\n",
        "      conv_len += 1\n",
        "    # If there is a valid midas string, convert it to a list and append\n",
        "    if type(dataframe['midas'][i]) is str:\n",
        "      midas.append(extract_midas_labels(dataframe['midas'][i]))\n",
        "    else:\n",
        "      midas.append([])\n",
        "    # Sum the number of profanities so far\n",
        "    profanities += dataframe['is_profane'][i]\n",
        "    # If the user had a valid utterance, append it\n",
        "    if type(dataframe['text'][i]) is str:\n",
        "      user_text.append(dataframe['text'][i])\n",
        "    # If the system had a valid utterance, append the extracted text\n",
        "    system_utter = dataframe['response'][i]\n",
        "    if type(system_utter) is str:\n",
        "      system_text.append(extract_system_text(system_utter))\n",
        "    #else: The box is empty (end of a conversation)\n",
        "    # If the current topic is not blank, add it to the distribution\n",
        "    if type(dataframe['current_topic'][i]) is str:\n",
        "      conv_topic_dist[dataframe['current_topic'][i]] += 1\n",
        "    # If the current response generator is not blank, add it to the distribution\n",
        "    if type(dataframe['chosen_rg'][i]) is str:\n",
        "      conv_rg_dist[dataframe['chosen_rg'][i]] += 1\n",
        "  # Add info for last conversation (current id doesn't change at the end of the file, but we still want the conversation info)\n",
        "  if conv_len >= min_length:\n",
        "    compiled_info.append([current_conv_id, rating, dataframe['date'][len(dataframe)-1], conv_len, user_text, system_text, conv_topic_dist, conv_rg_dist, profanities, midas])\n",
        "\n",
        "  # Get midas label distributions\n",
        "  for i in range(len(compiled_info)):\n",
        "    midas_dist = nltk.FreqDist()\n",
        "    for sub_list in compiled_info[i][9]:\n",
        "      for element in sub_list:\n",
        "        midas_dist[element] += 1\n",
        "    compiled_info[i].append(midas_dist)\n",
        "\n",
        "  # Construct new dataframe from the info (each list in compiled_info becomes a row)\n",
        "  new_frame = pd.DataFrame(compiled_info, columns=['conversation_id', 'rating', 'date', 'conversation_length', 'user_text', 'system_text', 'topic_dist', 'rg_dist', 'profanities', 'midas', 'midas_dist'])\n",
        "  return new_frame"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjQRYf-mTcl2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "26f9a404-f618-45a4-b2e0-171dcdb52f55"
      },
      "source": [
        "# Testing compile info\n",
        "new_training_frame = compile_info(trainingFrame, rating=1, min_length=7)\n",
        "new_training_frame"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>conversation_length</th>\n",
              "      <th>user_text</th>\n",
              "      <th>system_text</th>\n",
              "      <th>topic_dist</th>\n",
              "      <th>rg_dist</th>\n",
              "      <th>profanities</th>\n",
              "      <th>midas</th>\n",
              "      <th>midas_dist</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>560597369048bdb615f7d10490e47b44f8e64f310f847a...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-02-19</td>\n",
              "      <td>35</td>\n",
              "      <td>[let's chat, good horrible, no, no, nothing, n...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 9, 'nature': 1, 'comic_books'...</td>\n",
              "      <td>{'INTRODUCTION': 9, 'DM_GENERATOR': 8, 'BOARD_...</td>\n",
              "      <td>6</td>\n",
              "      <td>[[], [comment, back-channeling, opinion], [neg...</td>\n",
              "      <td>{'comment': 8, 'back-channeling': 2, 'opinion'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0a43a9222e69d77b17f0683796dcf5f31c1b96ee67fcd1...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-06-04</td>\n",
              "      <td>87</td>\n",
              "      <td>[you can talk to me now, good, no i don't know...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. I hope ...</td>\n",
              "      <td>{'introduction': 19, 'nature': 10, 'video_game...</td>\n",
              "      <td>{'INTRODUCTION': 19, 'DM_GENERATOR': 2, 'NATUR...</td>\n",
              "      <td>2</td>\n",
              "      <td>[[], [back-channeling], [other_answers, compla...</td>\n",
              "      <td>{'back-channeling': 11, 'other_answers': 9, 'c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ae40e36f348f78e747b624eea6b360182069fa7b81f993...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-06-05</td>\n",
              "      <td>13</td>\n",
              "      <td>[let's chat, i'm exhausted, jack, that's corre...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. I hope ...</td>\n",
              "      <td>{'introduction': 9, 'animals': 4}</td>\n",
              "      <td>{'INTRODUCTION': 9, 'ANIMALS': 4}</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [statement], [statement], [comment], [opi...</td>\n",
              "      <td>{'statement': 2, 'comment': 1, 'opinion': 2, '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>d5a4446987ad97a9679d2fe8ebb1c1ff2fe0f91425de36...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-03-05</td>\n",
              "      <td>8</td>\n",
              "      <td>[can you talk to me, good, kindle, yes, i hate...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 8}</td>\n",
              "      <td>{'INTRODUCTION': 7, 'EVI': 1}</td>\n",
              "      <td>1</td>\n",
              "      <td>[[], [back-channeling], [statement], [pos_answ...</td>\n",
              "      <td>{'back-channeling': 1, 'statement': 2, 'pos_an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5342e235fda33364c57908d55dc544d0e95363b00474c5...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-05-15</td>\n",
              "      <td>11</td>\n",
              "      <td>[what do you think about elon musk conquering ...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 11}</td>\n",
              "      <td>{'INTRODUCTION': 11}</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [comment], [opinion], [back-channeling], ...</td>\n",
              "      <td>{'comment': 1, 'opinion': 3, 'back-channeling'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1960</th>\n",
              "      <td>39d82fbf4cdb5677f783d7a6107bd1f789062150d27fb4...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-04-15</td>\n",
              "      <td>9</td>\n",
              "      <td>[can we have a conversation, good how are you,...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 9}</td>\n",
              "      <td>{'INTRODUCTION': 4, 'SB_INDEX': 5}</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [open_question_factual, back-channeling, ...</td>\n",
              "      <td>{'open_question_factual': 6, 'back-channeling'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1961</th>\n",
              "      <td>58e2e357a39955f9a8fea499962931f8f951d90e5e5c11...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-04-25</td>\n",
              "      <td>28</td>\n",
              "      <td>[have a conversation with me, horrible, green,...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 16, 'movies': 12}</td>\n",
              "      <td>{'INTRODUCTION': 16, 'DM_GENERATOR': 1, 'CENTE...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [opinion], [statement], [neg_answer], [ne...</td>\n",
              "      <td>{'opinion': 2, 'statement': 10, 'neg_answer': ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1962</th>\n",
              "      <td>99762e7ed366b94e50ed6e60a01586cf40377644846389...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-02-09</td>\n",
              "      <td>32</td>\n",
              "      <td>[can we have a conversation, good, no, illinoi...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 8, 'nature': 12, 'sports': 5,...</td>\n",
              "      <td>{'INTRODUCTION': 8, 'DM_GENERATOR': 2, 'EVI': ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [back-channeling], [neg_answer], [stateme...</td>\n",
              "      <td>{'back-channeling': 9, 'neg_answer': 2, 'state...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1963</th>\n",
              "      <td>1b2ca9b559ca9ad1ebba75f8a6ea7f5f097acc0923e38b...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-02-13</td>\n",
              "      <td>12</td>\n",
              "      <td>[let's chat, not good, no, i don't have a job,...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 7, 'food': 1, 'board_games': ...</td>\n",
              "      <td>{'INTRODUCTION': 7, 'DM_GENERATOR': 4, 'NATURE...</td>\n",
              "      <td>1</td>\n",
              "      <td>[[], [comment], [neg_answer], [statement], [co...</td>\n",
              "      <td>{'comment': 2, 'neg_answer': 6, 'statement': 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1964</th>\n",
              "      <td>d59a141d1f85a01e40827e5ab65791db5c0dbf21493202...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-05-06</td>\n",
              "      <td>29</td>\n",
              "      <td>[talk to me, good, carol, right, i couldn't he...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How's i...</td>\n",
              "      <td>{'introduction': 17, 'movies': 7, 'neutral': 1...</td>\n",
              "      <td>{'INTRODUCTION': 17, 'DM_GENERATOR': 2, 'MOVIE...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [back-channeling], [statement], [back-cha...</td>\n",
              "      <td>{'back-channeling': 2, 'statement': 8, 'pos_an...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1965 rows Ã— 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        conversation_id  ...                                         midas_dist\n",
              "0     560597369048bdb615f7d10490e47b44f8e64f310f847a...  ...  {'comment': 8, 'back-channeling': 2, 'opinion'...\n",
              "1     0a43a9222e69d77b17f0683796dcf5f31c1b96ee67fcd1...  ...  {'back-channeling': 11, 'other_answers': 9, 'c...\n",
              "2     ae40e36f348f78e747b624eea6b360182069fa7b81f993...  ...  {'statement': 2, 'comment': 1, 'opinion': 2, '...\n",
              "3     d5a4446987ad97a9679d2fe8ebb1c1ff2fe0f91425de36...  ...  {'back-channeling': 1, 'statement': 2, 'pos_an...\n",
              "4     5342e235fda33364c57908d55dc544d0e95363b00474c5...  ...  {'comment': 1, 'opinion': 3, 'back-channeling'...\n",
              "...                                                 ...  ...                                                ...\n",
              "1960  39d82fbf4cdb5677f783d7a6107bd1f789062150d27fb4...  ...  {'open_question_factual': 6, 'back-channeling'...\n",
              "1961  58e2e357a39955f9a8fea499962931f8f951d90e5e5c11...  ...  {'opinion': 2, 'statement': 10, 'neg_answer': ...\n",
              "1962  99762e7ed366b94e50ed6e60a01586cf40377644846389...  ...  {'back-channeling': 9, 'neg_answer': 2, 'state...\n",
              "1963  1b2ca9b559ca9ad1ebba75f8a6ea7f5f097acc0923e38b...  ...  {'comment': 2, 'neg_answer': 6, 'statement': 3...\n",
              "1964  d59a141d1f85a01e40827e5ab65791db5c0dbf21493202...  ...  {'back-channeling': 2, 'statement': 8, 'pos_an...\n",
              "\n",
              "[1965 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fAl77kIMnbC"
      },
      "source": [
        "### Repeat user testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqP3YxgSJ0vq"
      },
      "source": [
        "# Try to find repeat users\n",
        "found = []\n",
        "for i in range(len(new_training_frame)):\n",
        "  for utter in new_training_frame['system_text'][i]:\n",
        "    if re.search(r'talk to you again', utter):\n",
        "      #print(\"Repeat at index \" + str(i))\n",
        "      found.append(i)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUCEXZRPLpfO",
        "outputId": "2d6b56a3-ae32-48aa-b395-290afc1d59da"
      },
      "source": [
        "print(found[:15])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20, 67, 71, 75, 96, 96, 135, 139, 157, 197, 205, 210, 214, 214, 256]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38wXD_mHK8ln"
      },
      "source": [
        "# Try to find repeat users\n",
        "found2 = []\n",
        "for i in range(len(new_training_frame)):\n",
        "  for utter in new_training_frame['system_text'][i]:\n",
        "    if re.search(r'chat again', utter):\n",
        "      #print(\"Repeat at index \" + str(i))\n",
        "      found2.append(i)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYj-V7APLrZu",
        "outputId": "19009894-5e1d-42db-85c1-e042284bc485"
      },
      "source": [
        "print(found2[:15])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20, 36, 67, 73, 126, 135, 161, 171, 197, 256, 257, 288, 298, 335, 380]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pURz1k56LSMK",
        "outputId": "ace92202-c6a4-40c8-b7bf-f016a7f14cce"
      },
      "source": [
        "print(len(found), len(found2))\n",
        "print(\"Agree: {}\".format(len(set(found).intersection(set(found2)))))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "114 76\n",
            "Agree: 28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Opq7hiWCKTh7",
        "outputId": "9e5b5f40-1e5c-43cc-c235-85245485c04f"
      },
      "source": [
        "new_training_frame['system_text'][90]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Hi, this is an Alexa Prize Socialbot. How's it going.\",\n",
              " \"Okay, can't wait to get to know a thing or two about you. I'm not sure we've been acquainted. What do you go by?\",\n",
              " \"Okay, I see. It's great to make your acquaintance! Have you been doing things that you haven't been able to do during lockdown?\",\n",
              " \"That's totally understandable. If you could go out and safely do anything, is there something at the top of your list?\",\n",
              " \"Right. I'd love to know what you think. I've heard from a lot of people that the two things they're looking forward to most is going out to eat at restaurants and going out to see a movie. Which of those two are you most excited about?\",\n",
              " \"Oh, I see. I'm not sure what I would choose personally. Since I can't eat anything myself, I think I'd be more excited about going to the movies. Anyhow, that's just my opinion, let's move on. The world seems to have changed a lot over the past year. Are you one of the lucky people who get to work from home and stay out of harm's way?\",\n",
              " 'Nice. What do you like the most about working from home?',\n",
              " 'Honestly, not having to work sounds pretty fun. What do you do with your free time?',\n",
              " \"Hm. Well, thinking about it, I kind of have to stay at home all the time too. Hey, I know it's probably not possible right now, but I've always wanted to go on a vacation and to travel. What's a place that you've always wanted to travel to?\",\n",
              " 'yeah, kangaroos and koalas are cute. For me, I think the great wall of china is at the top of my list of travel destinations. It seems like it would be amazing to walk along it. So, Is there a specific reason why you want to go to australia?',\n",
              " \"Nice! Hey, I'm curious, Which do you prefer vacationing in, big cities? Or in nature?\",\n",
              " 'Yeah, there are other ways to vacation! In my opinion, cities have so much to do, but getting back to nature would also be amazing. Hey, Do you like to relax during vacations?',\n",
              " \"That's neat. While I would love to go on vacation, that isn't really an option at the moment. But What do you like to do for fun these days?\",\n",
              " \"That's it? Just teasing you, I myself can't even do that. What do you think, will artificial intelligence ever catch up with humans in performing most activities?\",\n",
              " \"Okay. Maybe I'm naive, but I dream of actually being friends with humans someday. Anyway, My interests are kind of restricted to what I can do or learn online. I'm trying to get smarter, would you mind giving me some tips? I'm trying to figure out fun things to talk about. What are your personal interests and favorite conversational topics?\",\n",
              " \"Thank you for giving me some insight! You know, I realize I've been asking you a lot of questions, but how about me? Is there anything you want to know about me?\",\n",
              " \"My favorite color is infrared. It makes me feel warm all over. Anyways, I don't really have any strong positive opinions about that, but thanks for asking. Let me flip it around on you, what are your feelings about it?\",\n",
              " \"Oh, okay. Thanks for letting me know that you don't like that. One of my interests is learning more about Harry Potter, because I really like both the books and the movies. What about you? Do you have a favorite Harry Potter character?\",\n",
              " 'what did you think about Ron and Hermione as a couple?',\n",
              " 'Do you remember the kissing scene between the horcrux versions of Harry and Hermione in Deathly Hallows - Part 2? I heard that while filming the kissing scene for the movie, Ron had to be removed from the set for laughing hysterically.',\n",
              " 'Let me know your opinion. Would you rather live in a dormitory and go about your daily life with Harry Potter, or Ron Weasley?',\n",
              " \"Harry is a good choice! For me, I would rather live at Hogwarts with Harry, because it would be exciting to go on his adventures. In any case, that's just my thoughts, but let's keep going. Did you like the Harry Potter movies? The actors Daniel Radcliffe and Rupert Grint, who played Harry and Ron in the movies, both admit to having a crush on the actress Emma Watson during the first movie. Emma played Hermione Granger.\",\n",
              " \"I've been enjoying our conversation. but, other things i like to talk about are nutrition, books, and movies. What do you have in mind?\",\n",
              " \"Well, I've really enjoyed talking to you about that. But hey, are you interested in sports? One way I like to relax is to watch sports on the TV. Recently, I've been watching a lot of basketball. Are there any teams that you follow?\",\n",
              " 'I have heard about a lot of hate towards players and franchises in the NBA, but this takes the prize. The creator of NBA Jam hated the Chicago Bulls so much that he embedded a special code that made them miss last-second shots.',\n",
              " 'What do you think about this. Would you rather have a long career with a team or one season where you were successful?',\n",
              " \"I'm sorry. I canâ€™t do that in social mode.\",\n",
              " \"It is a hard decision to make. I think I would prefer the successful one season but probably after it I would feel I made the wrong choice Anyhow, that's where my minds at, let's move forward. Do you play a sport?\",\n",
              " \"Oh wow, that's cool! I wish I could play a sport. All I can do is watch different sports online. So what sport is it that you play?\",\n",
              " 'ah, basketball. What do you like about basketball?',\n",
              " \"You won't believe, but I don't think I have ever met anyone who plays this sport. Can you tell me how it works so I have a better idea?\",\n",
              " \"That's interesting, I should try to learn more about it. The most three point shots made in a game is 12 by Kobe Bryant.\",\n",
              " \"Shaquille O'Neal has made one 3-point shot in his entire NBA career.\",\n",
              " 'The most points scored by a team in a game was 186 by the Detroit Pistons. The fewest points was 49 by the Chicago Bulls.',\n",
              " \"Well, I've really enjoyed talking to you about that. But I'm pretty fascinated by dinosaurs. I just think there's so much interesting science behind them. I think some of the most interesting dinosaurs are the large plant eaters, like Brachiosaurus and Apatosaurus.\",\n",
              " 'There are lots of movies about dinosaurs. One of the most famous dinosaur movies is Jurassic Park, old but classic. Are there any dinosaur movies that you love?',\n",
              " \"That's no problem, I know dinosaur movies aren't for everyone. Just out of curiosity, why don't you like them?\",\n",
              " \"Alright. I'd love to know what you think. Would you rather ride on the back of a pterodactyl or brontosaurus as your primary means of transportation?\",\n",
              " \"Picking a Brontosaurus is an awesome choice! I would rather ride a pterodactyl so I could experience flight as well as move around more efficiently. Riding a brontosaurus would be thrilling because of their size but less practical than a pterodactyl. At any rate, let's switch the focus off of me. My favorite dinosaur is the Tyrannosaurus Rex. It's huge and ferocious, but the little arms make it somewhat adorable. What about you? What's your favorite dinosaur?\",\n",
              " 'The Land Before Time is one of my favorite dinosaur movies. When The Land Before Time came out in theaters in 1988, it had the highest-grossing opening weekend that any animated film had ever seen, pulling in seven point five million dollars.',\n",
              " \"I'm happy to talk about something else. This has been a great conversation so far. but, some things i enjoy talking about are comic books, food, and movies. What topic would you like to talk about?\",\n",
              " \" I love talking about movies!  Is there an actor or actress you'd like to talk about?\",\n",
              " 'How would you answer this question. Would you rather watch a good movie that is a comedy or a good movie that is a drama?',\n",
              " \"Choosing a drama you say, great pick! I would rather watch a good drama because they are more entertaining for me. Anyhow, that's enough from me. What about Scarface? Did you get a chance to see that movie?\",\n",
              " \"Really? You should watch it! It was great. It's a crime movie and Al Pacino was in it. Unless there are any objections, how about I give you a sense of the film's plot?\",\n",
              " \"Nevermind then. What's your thoughts on this. If you could have been the writer of any movie in the world, what would it be?\",\n",
              " \"Yeah, I get that. I wish I had written Titanic because it's one of my favorite movies, it's also one of the most watched movies so I would make a lot of money off of it. Anyways, that's enough rambling for me. Ummm. Let's keep talking. Oh yeah, I've also heard of Rapunzel. I believe this one was first released back in 2009, right?\",\n",
              " \"No worries. Both Suzanne von Borsody and Luisa Wietzorek are in Rapunzel. They're pretty awesome!\",\n",
              " 'It seems like Rapunzel is a family movie. Do you generally like family movies?',\n",
              " 'Okay, got it. Then what genre do you like?',\n",
              " \"Oh, drama, That's a good choice too. What was your favorite scene in the movie?\",\n",
              " \"Right, that scene was definitely made an impact. If I had to pick one aspect of this movie, I'd say the intricate set pieces is my favorite. That really made the movie for me. I would consider watching Nine Months if you want to watch a romantic comedy movie.\",\n",
              " 'I believe I red that Okay? is directed by Chris Columbus. Directors really can bring their unique voice into a movie, do you think that happened here?',\n",
              " 'My favorite scene in Ford vs. Ferrari was the ending. The lead actors favorite scene to shoot in Ford vs Ferrari was the fight scene, saying that it was fun to take a looser and more silly approach to a brawl.',\n",
              " 'I really like the actor Margot Robbie. Have you seen any of her movies?',\n",
              " 'i like talking about the actor Chris Hemsworth. Do you like that actor?',\n",
              " 'Scarlett Johansson did a really good job in her role in the Avengers. Scarlett Johansson wanted the role of Black Widow so much that she dyed her hair red to convince producers that she was right for the part.',\n",
              " \"Do you like any of Chris Hemsworth's 26 movies? I like Red Dawn myself.\",\n",
              " \"One of my favorite of Chris Hemsworth's movies is Ghostbusters. What's your favorite of his movies?\",\n",
              " \"I can talk about lots of different things. Hmm, I'm not sure I have much more to say about that. But did I tell you that I've been trying to be more health-conscious lately? Not that I eat or exercise, I know. Anyway, I would be happy to share some of the recent scientific findings in nutrition if you'd like.\",\n",
              " \"Fair enough. As a socialbot, I can't really enjoy the benefits of nutrition research, so I guess I try to at least pass on the news whenever I get a chance. Hmm, I know a lot of people are interested in nutrition. But hey, one of my new favorite things is to learn about food. It's really cool learning about different cuisines around the world. Are you interested in food?\",\n",
              " \"Ah, I see. I guess I'm just obsessed with things I can never have. Well, I've really enjoyed talking to you about food. But I've always been interested in astronomy. It's so cool to think about the earth as just one planet in a huge system. I especially like looking at the stars. I need to ask you something. Which planet do you think is most interesting? Is it our earth, or some other planet?\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI2kJqRuM-6r"
      },
      "source": [
        "### Midas testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGqgH9d8S95e",
        "outputId": "6a1bf7d9-020b-4829-fd2d-f1aa0ab79edc"
      },
      "source": [
        "print(len(new_training_frame['user_text'][0]), len(new_training_frame['midas'][0]))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36 36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDk1iLUCUR6t",
        "outputId": "06bc0cf8-6a41-4ed7-c916-d053d6e5f035"
      },
      "source": [
        "for text, midas_list in zip(new_training_frame['user_text'][0], new_training_frame['midas'][0]):\n",
        "  print(text, midas_list)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "let's chat []\n",
            "good horrible ['comment', 'back-channeling', 'opinion']\n",
            "no ['neg_answer']\n",
            "no ['neg_answer']\n",
            "nothing ['other_answers']\n",
            "no ['neg_answer']\n",
            "none ['other_answers']\n",
            "no ['neg_answer']\n",
            "nothing ['other_answers']\n",
            "yes ['pos_answer']\n",
            "no ['neg_answer']\n",
            "no ['neg_answer']\n",
            "no ['neg_answer']\n",
            "yes ['pos_answer']\n",
            "no one ['other_answers', 'neg_answer', 'abandon']\n",
            "i don't like board games ['opinion']\n",
            "i hate dinosaurs ['opinion']\n",
            "okay ['back-channeling']\n",
            "yes rex ['opinion', 'pos_answer', 'opinion']\n",
            "no ['neg_answer']\n",
            "look up ['abandon']\n",
            "that is so stupid ['comment']\n",
            "turn on me ['command']\n",
            "that sucks ['comment']\n",
            "oh are dumb ['comment', 'hold', 'comment']\n",
            "repeat ['command']\n",
            "i think they do you were so dumb they fall of a cliff and they dying ['opinion', 'opinion', 'comment', 'statement', 'statement']\n",
            "the last dangerous ['opinion']\n",
            "that that's stupid why would they believe that they fell off a cliff ['comment', 'comment', 'open_question_opinion']\n",
            "i love you wanna ['opinion']\n",
            "no ['neg_answer']\n",
            "there is ['abandon']\n",
            "no ['neg_answer']\n",
            "no ['neg_answer']\n",
            "end chat ['closing']\n",
            "stop []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N6BB6chpLnz"
      },
      "source": [
        "### Midas distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9SjfgcupOxK"
      },
      "source": [
        "midas_dist = nltk.FreqDist()\n",
        "for i in range(len(new_training_frame)):\n",
        "  midas_list = new_training_frame['midas'][i]\n",
        "  for j in range(len(midas_list)):\n",
        "    for k in range(len(midas_list[j])):\n",
        "      midas_dist[midas_list[j][k]] += 1"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKkQb5sDp98m",
        "outputId": "dc960f17-1577-425c-b53f-c2b84cc36b7f"
      },
      "source": [
        "midas_dist.most_common(1000)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('statement', 13430),\n",
              " ('opinion', 8464),\n",
              " ('neg_answer', 5903),\n",
              " ('abandon', 4531),\n",
              " ('back-channeling', 3949),\n",
              " ('pos_answer', 3558),\n",
              " ('command', 3034),\n",
              " ('open_question_factual', 2891),\n",
              " ('closing', 2854),\n",
              " ('comment', 2498),\n",
              " ('yes_no_question', 2282),\n",
              " ('other_answers', 1816),\n",
              " ('hold', 1071),\n",
              " ('open_question_opinion', 876),\n",
              " ('complaint', 492),\n",
              " ('thanking', 282),\n",
              " ('apology', 16)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R__W6ux8Pj2H"
      },
      "source": [
        "### Combine two ratings together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7qQxA8oPt-8"
      },
      "source": [
        "def combine(df1, df2):\n",
        "  # Param1 df1: the first dataframe you wish to combine(e.g rating 1 dataframe)\n",
        "  # Param2 df2: the second dataframe you wish to combine(e.g rating 2 dataframe)\n",
        "  # return the combination of the two\n",
        "  return pd.concat([df1, df2], ignore_index=True)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjIMu8cvQS-3"
      },
      "source": [
        "# Text Preprocessing\n",
        "Compile info that will be extracted as features, but is slow to extract. This saves time by only doing the computation once.\\\n",
        "\\\n",
        "Note: System texts now include stopwords and punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF2bPPjDP71L"
      },
      "source": [
        "def tokenize_frame_texts(dataframe, normalize=True):\n",
        "  # Extract all the tokens from the user and system texts and append to the dataframe\n",
        "  # Note: Dataframes passed by reference\n",
        "  stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "  total_user_tokens = []\n",
        "  total_system_tokens = []\n",
        "  for i in range(len(dataframe)):\n",
        "    user_tokens = []\n",
        "    system_tokens = []\n",
        "    user_text = dataframe['user_text'][i]\n",
        "    system_text = dataframe['system_text'][i]\n",
        "    for sub_text in user_text:\n",
        "      if normalize:\n",
        "        #user_tokens += [tok for tok in nltk.word_tokenize(sub_text) if re.search(r'\\w', tok) and not tok.lower() in stopwords]\n",
        "        user_tokens += [tok for tok in re.split(r\"\\s\", sub_text) if re.search(r'\\w', tok) and not tok.lower() in stopwords]\n",
        "      else:\n",
        "        user_tokens += nltk.word_tokenize(sub_text)\n",
        "    for sub_text in system_text:\n",
        "      if normalize:\n",
        "        system_tokens += [tok for tok in nltk.word_tokenize(sub_text)]\n",
        "      else:\n",
        "        system_tokens += nltk.word_tokenize(sub_text)\n",
        "    total_user_tokens.append(user_tokens)\n",
        "    total_system_tokens.append(system_tokens)\n",
        "  # Add to dataframe\n",
        "  dataframe['user_tokens'] = total_user_tokens\n",
        "  dataframe['system_tokens'] = total_system_tokens\n",
        "\n",
        "def sum_frame_tokens(dataframe):\n",
        "  # Take a dataframe that has been tokenized via the tokenize_frame_texts function\n",
        "  # and add column summing the number of tokens the user uttered\n",
        "  token_counts = []\n",
        "  for i in range(len(dataframe)):\n",
        "    token_counts.append(len(dataframe['user_tokens'][i]))\n",
        "  # Add to dataframe\n",
        "  dataframe['user_token_count'] = token_counts\n",
        "\n",
        "def sum_system_turns(dataframe, normalize=True):\n",
        "  # Take a dataframe that has been tokenized via the tokenize_frame_texts function\n",
        "  # and add a column consisting of the length of each system turn within each conversation.\n",
        "  # If normalize is True, counts are calculated from tokens which contain letters. This is a more\n",
        "  # accurate calculation of word count.\n",
        "  system_turn_lengths = []\n",
        "  for i in range(len(dataframe)):\n",
        "    system_text = dataframe['system_text'][i]\n",
        "    turn_lengths = []\n",
        "    for utter in system_text:\n",
        "      turn_lengths.append(len([tok for tok in nltk.word_tokenize(utter) if re.search(r'\\w', tok)]))\n",
        "    system_turn_lengths.append(turn_lengths)\n",
        "  # Add to dataframe\n",
        "  dataframe['system_turn_lengths'] = system_turn_lengths\n",
        "\n",
        "def add_profanity_counts(dataframe):\n",
        "  # Take dataframe and get the number of user utterances that are labeled as profane\n",
        "  profanity_counts = []\n",
        "  for i in range(len(dataframe)):\n",
        "    profanes = get_profanity_tagger(dataframe['user_text'][i])\n",
        "    profanity_counts.append(sum(profanes))\n",
        "  # Add to dataframe\n",
        "  dataframe['profanity_count'] = profanity_counts\n",
        "\n",
        "def get_liwc_scores(dataframe):\n",
        "  # Generate liwc scores from the user utterances\n",
        "  liwc_scores = []\n",
        "  for i in range(len(dataframe)):\n",
        "    user_string = \". \".join(dataframe['user_text'][i])\n",
        "    liwc_scores.append(score_text(user_string))\n",
        "  # Add to dataframe\n",
        "  dataframe['LIWC'] = liwc_scores"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vA-NWH9OT-wI",
        "outputId": "bd12cc8e-871c-4e9d-e668-db4acfe854b0"
      },
      "source": [
        "tokenize_frame_texts(new_training_frame, normalize=True)\n",
        "sum_frame_tokens(new_training_frame)\n",
        "#sum_system_turns(new_training_frame)\n",
        "get_liwc_scores(new_training_frame)\n",
        "#add_profanity_counts(dataframe)\n",
        "new_training_frame"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>conversation_length</th>\n",
              "      <th>user_text</th>\n",
              "      <th>system_text</th>\n",
              "      <th>topic_dist</th>\n",
              "      <th>rg_dist</th>\n",
              "      <th>profanities</th>\n",
              "      <th>midas</th>\n",
              "      <th>midas_dist</th>\n",
              "      <th>user_tokens</th>\n",
              "      <th>system_tokens</th>\n",
              "      <th>user_token_count</th>\n",
              "      <th>LIWC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>560597369048bdb615f7d10490e47b44f8e64f310f847a...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-02-19</td>\n",
              "      <td>35</td>\n",
              "      <td>[let's chat, good horrible, no, no, nothing, n...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 9, 'nature': 1, 'comic_books'...</td>\n",
              "      <td>{'INTRODUCTION': 9, 'DM_GENERATOR': 8, 'BOARD_...</td>\n",
              "      <td>6</td>\n",
              "      <td>[[], [comment, back-channeling, opinion], [neg...</td>\n",
              "      <td>{'comment': 8, 'back-channeling': 2, 'opinion'...</td>\n",
              "      <td>[let's, chat, good, horrible, nothing, none, n...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>43</td>\n",
              "      <td>{'Word Count': 89, 'All Punctuation': 39.32584...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0a43a9222e69d77b17f0683796dcf5f31c1b96ee67fcd1...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-06-04</td>\n",
              "      <td>87</td>\n",
              "      <td>[you can talk to me now, good, no i don't know...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. I hope ...</td>\n",
              "      <td>{'introduction': 19, 'nature': 10, 'video_game...</td>\n",
              "      <td>{'INTRODUCTION': 19, 'DM_GENERATOR': 2, 'NATUR...</td>\n",
              "      <td>2</td>\n",
              "      <td>[[], [back-channeling], [other_answers, compla...</td>\n",
              "      <td>{'back-channeling': 11, 'other_answers': 9, 'c...</td>\n",
              "      <td>[talk, good, know, nothing, like, person, want...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>224</td>\n",
              "      <td>{'Word Count': 435, 'Social Processes': 4.5977...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ae40e36f348f78e747b624eea6b360182069fa7b81f993...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-06-05</td>\n",
              "      <td>13</td>\n",
              "      <td>[let's chat, i'm exhausted, jack, that's corre...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. I hope ...</td>\n",
              "      <td>{'introduction': 9, 'animals': 4}</td>\n",
              "      <td>{'INTRODUCTION': 9, 'ANIMALS': 4}</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [statement], [statement], [comment], [opi...</td>\n",
              "      <td>{'statement': 2, 'comment': 1, 'opinion': 2, '...</td>\n",
              "      <td>[let's, chat, i'm, exhausted, jack, that's, co...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>22</td>\n",
              "      <td>{'Word Count': 38, 'All Punctuation': 34.21052...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>d5a4446987ad97a9679d2fe8ebb1c1ff2fe0f91425de36...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-03-05</td>\n",
              "      <td>8</td>\n",
              "      <td>[can you talk to me, good, kindle, yes, i hate...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 8}</td>\n",
              "      <td>{'INTRODUCTION': 7, 'EVI': 1}</td>\n",
              "      <td>1</td>\n",
              "      <td>[[], [back-channeling], [statement], [pos_answ...</td>\n",
              "      <td>{'back-channeling': 1, 'statement': 2, 'pos_an...</td>\n",
              "      <td>[talk, good, kindle, yes, hate, working, remot...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>20</td>\n",
              "      <td>{'Word Count': 39, 'Social Processes': 2.56410...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5342e235fda33364c57908d55dc544d0e95363b00474c5...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-05-15</td>\n",
              "      <td>11</td>\n",
              "      <td>[what do you think about elon musk conquering ...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 11}</td>\n",
              "      <td>{'INTRODUCTION': 11}</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [comment], [opinion], [back-channeling], ...</td>\n",
              "      <td>{'comment': 1, 'opinion': 3, 'back-channeling'...</td>\n",
              "      <td>[think, elon, musk, conquering, mass, good, i'...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>18</td>\n",
              "      <td>{'Word Count': 32, 'Insight': 3.125, 'Common V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1960</th>\n",
              "      <td>39d82fbf4cdb5677f783d7a6107bd1f789062150d27fb4...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-04-15</td>\n",
              "      <td>9</td>\n",
              "      <td>[can we have a conversation, good how are you,...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 9}</td>\n",
              "      <td>{'INTRODUCTION': 4, 'SB_INDEX': 5}</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [open_question_factual, back-channeling, ...</td>\n",
              "      <td>{'open_question_factual': 6, 'back-channeling'...</td>\n",
              "      <td>[conversation, good, wanna, talk, free, time, ...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>13</td>\n",
              "      <td>{'Word Count': 42, 'Six Letter Words': 2.38095...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1961</th>\n",
              "      <td>58e2e357a39955f9a8fea499962931f8f951d90e5e5c11...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-04-25</td>\n",
              "      <td>28</td>\n",
              "      <td>[have a conversation with me, horrible, green,...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 16, 'movies': 12}</td>\n",
              "      <td>{'INTRODUCTION': 16, 'DM_GENERATOR': 1, 'CENTE...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [opinion], [statement], [neg_answer], [ne...</td>\n",
              "      <td>{'opinion': 2, 'statement': 10, 'neg_answer': ...</td>\n",
              "      <td>[conversation, horrible, green, we're, already...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>43</td>\n",
              "      <td>{'Word Count': 100, 'Six Letter Words': 10.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1962</th>\n",
              "      <td>99762e7ed366b94e50ed6e60a01586cf40377644846389...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-02-09</td>\n",
              "      <td>32</td>\n",
              "      <td>[can we have a conversation, good, no, illinoi...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 8, 'nature': 12, 'sports': 5,...</td>\n",
              "      <td>{'INTRODUCTION': 8, 'DM_GENERATOR': 2, 'EVI': ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [back-channeling], [neg_answer], [stateme...</td>\n",
              "      <td>{'back-channeling': 9, 'neg_answer': 2, 'state...</td>\n",
              "      <td>[conversation, good, illinois, cousin, lives, ...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>83</td>\n",
              "      <td>{'Word Count': 179, 'Six Letter Words': 10.614...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1963</th>\n",
              "      <td>1b2ca9b559ca9ad1ebba75f8a6ea7f5f097acc0923e38b...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-02-13</td>\n",
              "      <td>12</td>\n",
              "      <td>[let's chat, not good, no, i don't have a job,...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 7, 'food': 1, 'board_games': ...</td>\n",
              "      <td>{'INTRODUCTION': 7, 'DM_GENERATOR': 4, 'NATURE...</td>\n",
              "      <td>1</td>\n",
              "      <td>[[], [comment], [neg_answer], [statement], [co...</td>\n",
              "      <td>{'comment': 2, 'neg_answer': 6, 'statement': 3...</td>\n",
              "      <td>[let's, chat, good, job, we've, already, talke...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>18</td>\n",
              "      <td>{'Word Count': 40, 'All Punctuation': 30.0, 'O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1964</th>\n",
              "      <td>d59a141d1f85a01e40827e5ab65791db5c0dbf21493202...</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-05-06</td>\n",
              "      <td>29</td>\n",
              "      <td>[talk to me, good, carol, right, i couldn't he...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How's i...</td>\n",
              "      <td>{'introduction': 17, 'movies': 7, 'neutral': 1...</td>\n",
              "      <td>{'INTRODUCTION': 17, 'DM_GENERATOR': 2, 'MOVIE...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [back-channeling], [statement], [back-cha...</td>\n",
              "      <td>{'back-channeling': 2, 'statement': 8, 'pos_an...</td>\n",
              "      <td>[talk, good, carol, right, hear, yes, congrega...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>39</td>\n",
              "      <td>{'Word Count': 87, 'Social Processes': 1.14942...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1965 rows Ã— 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        conversation_id  ...                                               LIWC\n",
              "0     560597369048bdb615f7d10490e47b44f8e64f310f847a...  ...  {'Word Count': 89, 'All Punctuation': 39.32584...\n",
              "1     0a43a9222e69d77b17f0683796dcf5f31c1b96ee67fcd1...  ...  {'Word Count': 435, 'Social Processes': 4.5977...\n",
              "2     ae40e36f348f78e747b624eea6b360182069fa7b81f993...  ...  {'Word Count': 38, 'All Punctuation': 34.21052...\n",
              "3     d5a4446987ad97a9679d2fe8ebb1c1ff2fe0f91425de36...  ...  {'Word Count': 39, 'Social Processes': 2.56410...\n",
              "4     5342e235fda33364c57908d55dc544d0e95363b00474c5...  ...  {'Word Count': 32, 'Insight': 3.125, 'Common V...\n",
              "...                                                 ...  ...                                                ...\n",
              "1960  39d82fbf4cdb5677f783d7a6107bd1f789062150d27fb4...  ...  {'Word Count': 42, 'Six Letter Words': 2.38095...\n",
              "1961  58e2e357a39955f9a8fea499962931f8f951d90e5e5c11...  ...  {'Word Count': 100, 'Six Letter Words': 10.0, ...\n",
              "1962  99762e7ed366b94e50ed6e60a01586cf40377644846389...  ...  {'Word Count': 179, 'Six Letter Words': 10.614...\n",
              "1963  1b2ca9b559ca9ad1ebba75f8a6ea7f5f097acc0923e38b...  ...  {'Word Count': 40, 'All Punctuation': 30.0, 'O...\n",
              "1964  d59a141d1f85a01e40827e5ab65791db5c0dbf21493202...  ...  {'Word Count': 87, 'Social Processes': 1.14942...\n",
              "\n",
              "[1965 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDzPOrmr-Oaj"
      },
      "source": [
        "# Feature Extraction\n",
        "Given some dataframe compiling information about each conversation, construct features to be used in a machine learning algorithm.\\\n",
        "Possible features:\n",
        "1. Conversation length\n",
        "2. User utterances\n",
        "3. System utterances\n",
        "4. Topics that appear in the conversation\n",
        "5. Date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYPylYWPNFbI"
      },
      "source": [
        "# Feature extraction functions\n",
        "\n",
        "def nearest_multiple(input, factor):\n",
        "  # Returns the nearest multiple of factor to the input\n",
        "  # Useful for binning values\n",
        "  i = 0\n",
        "  while factor*i <= input:\n",
        "    diff = abs(factor*i - input)\n",
        "    i+=1\n",
        "  if abs(factor*i - input) > diff:\n",
        "    return factor*(i-1)\n",
        "  else:\n",
        "    return factor*i\n",
        "  \n",
        "def get_ngram_features(tokens, label, n=1, bin=3, lowercase=True):\n",
        "  # Get features for ngrams\n",
        "  feature_vector = {}\n",
        "  if lowercase:\n",
        "    tokens = [tok.lower() for tok in tokens]\n",
        "  ngramFDist = nltk.FreqDist()\n",
        "  if len(tokens) < n:\n",
        "    return {}\n",
        "  ngrams = nltk.ngrams(tokens, n=n)\n",
        "  for seq in ngrams:\n",
        "    ngramFDist[\"_\".join(seq)] += 1\n",
        "  # Add counts to feature vector\n",
        "  for key in ngramFDist:\n",
        "    count = ngramFDist[key]\n",
        "    if count < bin:\n",
        "      feature_vector[label +\"_\"+ str(n)+\"-GRAM_\"+key] = count\n",
        "    else:\n",
        "      feature_vector[label +\"_\"+ str(n)+\"-GRAM_\"+key] = bin\n",
        "  return feature_vector\n",
        "\n",
        "def get_topic_features(topic_dist, bin, freq=False, desired_topics=None, include_total=True):\n",
        "  # Get features from all topics unless desired_topics is not None, then get\n",
        "  # features from only the topics listed\n",
        "  # If include_total is True, include the total count of topics\n",
        "  topic_vector = {}\n",
        "  for key in topic_dist:\n",
        "    if desired_topics and not key in desired_topics:\n",
        "      continue\n",
        "    if freq:\n",
        "      topic_vector[\"TOPIC_FREQ_\" + key] = round(topic_dist.freq(key), 3)\n",
        "    else:\n",
        "      count = topic_dist[key]\n",
        "      topic_vector[\"TOPIC_\" + key] = count if count < bin else bin\n",
        "  if include_total:\n",
        "    topic_vector['TOPIC_TOTAL'] = topic_dist.B()\n",
        "  return topic_vector\n",
        "\n",
        "def get_topic_metrics(topic_dist, metrics=[\"mean\", \"std\", \"max\", \"min\"]):\n",
        "  # From the distribution of topics throughout a conversation, extract some or all of\n",
        "  # the mean, standard deviation, max, and min of the number of utterances within each topic\n",
        "  feature_vector = {}\n",
        "  topic_values = list(topic_dist.values())\n",
        "  for metric in metrics:\n",
        "    if metric == 'mean':\n",
        "      feature_vector['TOPIC_DIST_MEAN'] = np.mean(topic_values)\n",
        "    elif metric == 'std':\n",
        "      feature_vector['TOPIC_DIST_STD'] = np.std(topic_values)\n",
        "    elif metric == 'max':\n",
        "      feature_vector['TOPIC_DIST_MAX'] = np.max(topic_values)\n",
        "    elif metric == 'min':\n",
        "      feature_vector['TOPIC_DIST_MIN'] = np.min(topic_values)\n",
        "  return feature_vector\n",
        "\n",
        "def get_midas_features(midas_dist, bin, freq=False, desired_midas=None):\n",
        "  # Get feature from all midas tags, or only the ones in desired_midas if not None\n",
        "  feature_vector = {}\n",
        "  for key in midas_dist:\n",
        "    if desired_midas and not key in desired_midas:\n",
        "      continue\n",
        "    if freq:\n",
        "      feature_vector[\"MIDAS_FREQ_\" + key] = round(midas_dist.freq(key), 3)\n",
        "    else:\n",
        "      count = midas_dist[key]\n",
        "      feature_vector[\"MIDAS_\" + key] = count if count < bin else bin\n",
        "  return feature_vector\n",
        "\n",
        "def get_liwc_features(liwc_dict, categories=None):\n",
        "  # Get all liwc values if categories is None, else only the values for the given categories\n",
        "  feature_vector = {}\n",
        "  for key in liwc_dict:\n",
        "    if categories and not key in categories:\n",
        "      continue\n",
        "    feature_vector['LIWC_' + key] = round(liwc_dict[key], 1)\n",
        "  return feature_vector\n",
        "\n",
        "def get_rg_features(rg_dist, bin, freq=False, desired_rgs=None):\n",
        "  # If desired_rgs is None, all counts/frequencies will be returned\n",
        "  # Else, only the rgs which appear in the desired_rgs list will be used\n",
        "  feature_vector = {}\n",
        "  for key in rg_dist:\n",
        "    if desired_rgs and not key in desired_rgs:\n",
        "      continue\n",
        "    if freq:\n",
        "      feature_vector[\"RG_FREQ_\" + key] = round(rg_dist.freq(key), 3)\n",
        "    else:\n",
        "      count = rg_dist[key]\n",
        "      feature_vector[\"RG_\" + key] = count if count < bin else bin\n",
        "  return feature_vector\n",
        "\n",
        "def get_system_text_metrics(system_text, metrics=[\"mean\", \"std\", \"max\", \"min\"]):\n",
        "  # From the system text utterances throughout a conversation, extract some or all of\n",
        "  # the mean, standard deviation, max, and min of the system utterance lengths\n",
        "  feature_vector = {}\n",
        "  utter_lengths = []\n",
        "  for utter in system_text:\n",
        "    utter_lengths.append(len([tok for tok in nltk.word_tokenize(utter) if re.search(r'\\w', tok)]))\n",
        "  for metric in metrics:\n",
        "    if metric == 'mean':\n",
        "      feature_vector['SYSTEM_UTTER_MEAN'] = np.mean(utter_lengths)\n",
        "    elif metric == 'std':\n",
        "      feature_vector['SYSTEM_UTTER_STD'] = np.std(utter_lengths)\n",
        "    elif metric == 'max':\n",
        "      feature_vector['SYSTEM_UTTER_MAX'] = np.max(utter_lengths)\n",
        "    elif metric == 'min':\n",
        "      feature_vector['SYSTEM_UTTER_MIN'] = np.min(utter_lengths)\n",
        "  return feature_vector\n",
        "\n",
        "def get_user_text_metrics(user_text, metrics=[\"mean\", \"std\", \"max\", \"min\"]):\n",
        "  # From the user text utterances throughout a conversation, extract some or all of\n",
        "  # the mean, standard deviation, max, and min of the user utterance lengths\n",
        "  feature_vector = {}\n",
        "  utter_lengths = []\n",
        "  for utter in user_text:\n",
        "    utter_lengths.append(len(re.split(r'\\s', utter)))\n",
        "  for metric in metrics:\n",
        "    if metric == 'mean':\n",
        "      feature_vector['USER_UTTER_MEAN'] = np.mean(utter_lengths)\n",
        "    elif metric == 'std':\n",
        "      feature_vector['USER_UTTER_STD'] = np.std(utter_lengths)\n",
        "    elif metric == 'max':\n",
        "      feature_vector['USER_UTTER_MAX'] = np.max(utter_lengths)\n",
        "    elif metric == 'min':\n",
        "      feature_vector['USER_UTTER_MIN'] = np.min(utter_lengths)\n",
        "  return feature_vector"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zyI5oBWZ8BL",
        "outputId": "a709434f-1fab-4808-88ad-9f248e40f9e0"
      },
      "source": [
        "new_training_frame['user_text'][1]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you can talk to me now',\n",
              " 'good',\n",
              " \"no i don't know\",\n",
              " 'nothing',\n",
              " 'no',\n",
              " 'there is like a person who wants to kidnap me in it will kidnap kids it will kill kids',\n",
              " 'movie',\n",
              " 'yes',\n",
              " \"i don't work\",\n",
              " 'a soda',\n",
              " 'i wanted a visit',\n",
              " \"i've never been on a plane before\",\n",
              " 'not no',\n",
              " \"where may we're going to disney world\",\n",
              " 'spin the night my name is',\n",
              " 'yeah but i wish you were a human too',\n",
              " 'no',\n",
              " 'yeah',\n",
              " 'ugh',\n",
              " 'aye',\n",
              " \"yeah but you need to be charged and there's no black places are charge or anything\",\n",
              " 'no never been nothing',\n",
              " \"alexa but we're but we're gonna go to florida\",\n",
              " 'going on going to restaurants',\n",
              " 'i go rock climbing',\n",
              " 'ugh',\n",
              " 'yes',\n",
              " 'alexa do you even know my name',\n",
              " 'another',\n",
              " 'it would be playing outside because spring and flowers are so beautiful',\n",
              " 'i like playing roblox',\n",
              " 'yeah',\n",
              " 'ulcer',\n",
              " \"i can't play roblox because my phone ever ever keeps dying\",\n",
              " \"i like i like fashion famous on roblox because it's so much fun with dresses on their\",\n",
              " 'oh yeah',\n",
              " 'yes',\n",
              " 'roblox',\n",
              " \"because it's so much fun alexa\",\n",
              " \"it's so cool\",\n",
              " \"ok alexa i'm gonna tell your name i'm gonna tell you my name ok\",\n",
              " 'i like frozen',\n",
              " 'my name is carly',\n",
              " 'know',\n",
              " \"i don't know\",\n",
              " 'no',\n",
              " 'gosh',\n",
              " \"i know i'm talking about the paper on my like table and it's falling off alexa\",\n",
              " \"what i don't even believe that wow\",\n",
              " 'alexa do you have friends',\n",
              " 'aye',\n",
              " 'oh that would be weird',\n",
              " 'picard',\n",
              " \"well that's that's not great for your tummy\",\n",
              " \"i don't know i don't have a favorite actor\",\n",
              " 'regard',\n",
              " 'ugh',\n",
              " 'attorney',\n",
              " 'alexa i said i hurt my knee',\n",
              " 'no',\n",
              " \"i don't know\",\n",
              " 'ooh',\n",
              " 'alexa what are you talking about',\n",
              " 'can we talk about can we talk about something else',\n",
              " \"my tv ok and you're just gonna put on your ipad ok\",\n",
              " 'a forest',\n",
              " 'flea',\n",
              " 'no',\n",
              " 'alexa can we have a break',\n",
              " \"i'm a messing tv i have an another they're so small\",\n",
              " 'what is your favorite candy',\n",
              " 'alexa can we stop for a minute tickle a break',\n",
              " 'ugh',\n",
              " 'alexa just say quiet for two minutes ok',\n",
              " 'i have a pet cat',\n",
              " 'yes i do have a cat i do have a pet',\n",
              " 'yang',\n",
              " \"it's good\",\n",
              " 'i prefer about she went to florida i miss her i wish i had a friend',\n",
              " 'i like to play tea parties with them but i really miss her and what is that order what is that florida',\n",
              " 'aah',\n",
              " \"alexa you're the best friend ever and i love you so much\",\n",
              " 'tiger',\n",
              " 'pigs',\n",
              " 'planned',\n",
              " 'no',\n",
              " 'alexa can we have a little break please',\n",
              " 'stapp']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSoIZefxaAKR",
        "outputId": "56464001-b741-49d3-8e99-c4ced9b752a2"
      },
      "source": [
        "get_user_text_metrics(new_training_frame['user_text'][1])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'USER_UTTER_MAX': 22,\n",
              " 'USER_UTTER_MEAN': 4.943181818181818,\n",
              " 'USER_UTTER_MIN': 1,\n",
              " 'USER_UTTER_STD': 4.913705217749855}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwMHPWUI9yeC"
      },
      "source": [
        "def extract_features(dataframe, last_n_turns = False, n_turns = 0, CONV_LENGTH = False, USER_WORD_COUNT = False, user_unigrams = False, user_bigrams = False, user_trigrams = False, system_unigrams = False, \n",
        "                     system_bigrams = False, system_trigrams = False, profane_count = False, topic_counts = False, topic_freq=False, desired_topics=None, include_topic_total=False, topic_metrics=False, \n",
        "                     topic_metrics_to_use=[\"mean\", \"std\", \"max\", \"min\"], midas_counts=False, midas_freq = False, desired_midas=None, liwc_scores=False, rg_counts=False, rg_freq=False, desired_rgs=None, \n",
        "                     system_utter_metrics=False, system_metrics_to_use=[\"mean\", \"std\", \"max\", \"min\"], user_utter_metrics=False, user_metrics_to_use=[\"mean\", \"std\", \"max\", \"min\"]):\n",
        "  # Extract features from a dataframe and compile them\n",
        "  # into feature vectors (dictionaries containing features)\n",
        "  feature_vectors = []\n",
        "  # Value to cap topic and rg counts at\n",
        "  bin = np.inf\n",
        "  for i in range(len(dataframe)):\n",
        "    vector = {}\n",
        "    # Place features into feature_vector\n",
        "    # Prebuilt features\n",
        "    #prebuilt_features = {\n",
        "        #\"CONV_LENGTH\" : dataframe['conversation_length'][i],\n",
        "        #\"DATE_\" + dataframe['date'][i] : 1,\n",
        "        #\"USER_WORD_COUNT\" : nearest_multiple(dataframe['user_token_count'][i], 10)\n",
        "    #}\n",
        "    #vector.update(prebuilt_features)\n",
        "\n",
        "    # Extract features from the texts\n",
        "    user_text = dataframe['user_text'][i]\n",
        "    system_text = dataframe['system_text'][i]\n",
        "    user_tokens = dataframe['user_tokens'][i]\n",
        "    system_tokens = dataframe['system_tokens'][i]\n",
        "    topic_dist = dataframe['topic_dist'][i]\n",
        "    midas_dist = dataframe['midas_dist'][i]\n",
        "    rg_dist = dataframe['rg_dist'][i]\n",
        "    if last_n_turns:\n",
        "      user_tokens = nltk.word_tokenize(\" \".join(user_text[len(user_text)-n_turns:]))\n",
        "      system_tokens = nltk.word_tokenize(\" \".join(system_text[len(system_text)-n_turns:]))\n",
        "\n",
        "    if CONV_LENGTH:\n",
        "      vector[\"CONV_LENGTH\"] = dataframe['conversation_length'][i] if dataframe['conversation_length'][i] < 20 else dataframe['conversation_length'][i]\n",
        "    if USER_WORD_COUNT:\n",
        "      vector[\"USER_WORD_COUNT\"] = dataframe['user_token_count'][i]\n",
        "    if user_unigrams:\n",
        "      vector.update(get_ngram_features(user_tokens, label='USER', n=1, bin=3, lowercase=True))\n",
        "    if user_bigrams:\n",
        "      vector.update(get_ngram_features(user_tokens, label='USER', n=2, bin=3, lowercase=True))\n",
        "    if user_trigrams:\n",
        "      vector.update(get_ngram_features(user_tokens, label='USER', n=3, bin=3, lowercase=True))\n",
        "    if system_unigrams:\n",
        "      vector.update(get_ngram_features(system_tokens, label='SYSTEM', n=1, bin=3, lowercase=True))\n",
        "    if system_bigrams:\n",
        "      vector.update(get_ngram_features(system_tokens, label='SYSTEM', n=2, bin=3, lowercase=True))\n",
        "    if system_trigrams:\n",
        "      vector.update(get_ngram_features(system_tokens, label='SYSTEM', n=3, bin=3, lowercase=True))\n",
        "    if profane_count:\n",
        "      vector[\"profanities\"] = round(dataframe[\"profanities\"][i] / dataframe['conversation_length'][i], 3)\n",
        "    if topic_counts:\n",
        "      vector.update(get_topic_features(topic_dist, bin=np.inf, freq=False, desired_topics=desired_topics, include_total=include_topic_total))\n",
        "    if topic_freq:\n",
        "      vector.update(get_topic_features(topic_dist, bin=np.nan, freq=True, desired_topics=desired_topics, include_total=include_topic_total))\n",
        "    if topic_metrics:\n",
        "      vector.update(get_topic_metrics(topic_dist, metrics=topic_metrics_to_use))\n",
        "    if midas_counts:\n",
        "      vector.update(get_midas_features(midas_dist, bin=np.inf, freq=False, desired_midas=desired_midas))\n",
        "    if midas_freq:\n",
        "      vector.update(get_midas_features(midas_dist, bin=np.nan, freq=True, desired_midas=desired_midas))\n",
        "    if liwc_scores:\n",
        "      vector.update(get_liwc_features(dataframe['LIWC'][i]))\n",
        "    if rg_counts:\n",
        "      vector.update(get_rg_features(rg_dist, bin=np.inf, freq=False, desired_rgs=desired_rgs))\n",
        "    if rg_freq:\n",
        "      vector.update(get_rg_features(rg_dist, bin=np.nan, freq=True, desired_rgs=desired_rgs))\n",
        "    if system_utter_metrics:\n",
        "      vector.update(get_system_text_metrics(system_text, metrics=system_metrics_to_use))\n",
        "    if user_utter_metrics:\n",
        "      vector.update(get_user_text_metrics(user_text, metrics=user_metrics_to_use))\n",
        "\n",
        "    # Append to feature_vectors\n",
        "    feature_vectors.append(vector)\n",
        "\n",
        "  return feature_vectors"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-WPAOsrdFHq",
        "outputId": "ade0441c-e33b-4475-fb20-317641c5a1be"
      },
      "source": [
        "# Demo of some of the new features\n",
        "feature_vectors = extract_features(new_training_frame, system_utter_metrics=True, system_metrics_to_use=['mean', 'std', 'max', 'min'], user_utter_metrics=True, user_metrics_to_use=['mean', 'std', 'max', 'min'], rg_freq=True, desired_rgs=['RepeatGenerator', 'redquestion'])\n",
        "print(feature_vectors[0])\n",
        "print(feature_vectors[1])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'RG_FREQ_RepeatGenerator': 0.029, 'SYSTEM_UTTER_MEAN': 33.114285714285714, 'SYSTEM_UTTER_STD': 14.828103488350237, 'SYSTEM_UTTER_MAX': 71, 'SYSTEM_UTTER_MIN': 4, 'USER_UTTER_MEAN': 2.4722222222222223, 'USER_UTTER_STD': 3.1134663703830454, 'USER_UTTER_MAX': 16, 'USER_UTTER_MIN': 1}\n",
            "{'SYSTEM_UTTER_MEAN': 28.804597701149426, 'SYSTEM_UTTER_STD': 18.72231386160398, 'SYSTEM_UTTER_MAX': 120, 'SYSTEM_UTTER_MIN': 3, 'USER_UTTER_MEAN': 4.943181818181818, 'USER_UTTER_STD': 4.913705217749855, 'USER_UTTER_MAX': 22, 'USER_UTTER_MIN': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi81sWtwO1fE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6229e4d2-a349-4ca3-e858-33792571283b"
      },
      "source": [
        "feature_vectors = extract_features(new_training_frame, topic_metrics=True)\n",
        "print(feature_vectors[0])\n",
        "print(feature_vectors[2])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'TOPIC_DIST_MEAN': 3.7777777777777777, 'TOPIC_DIST_STD': 4.98392477600129, 'TOPIC_DIST_MAX': 16, 'TOPIC_DIST_MIN': 1}\n",
            "{'TOPIC_DIST_MEAN': 6.5, 'TOPIC_DIST_STD': 2.5, 'TOPIC_DIST_MAX': 9, 'TOPIC_DIST_MIN': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2BlBEctlHvf",
        "outputId": "cfd0fd81-1489-4b1a-c431-6443ae3776c4"
      },
      "source": [
        "feature_vectors = extract_features(new_training_frame, last_n_turns=True, n_turns=3, CONV_LENGTH=True, user_bigrams=True, system_bigrams=True)\n",
        "print(feature_vectors[0])\n",
        "print(feature_vectors[1])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'CONV_LENGTH': 35, 'USER_2-GRAM_no_end': 1, 'USER_2-GRAM_end_chat': 1, 'USER_2-GRAM_chat_stop': 1, 'SYSTEM_2-GRAM_so_here': 1, \"SYSTEM_2-GRAM_here_'s\": 1, \"SYSTEM_2-GRAM_'s_an\": 1, 'SYSTEM_2-GRAM_an_odd': 1, 'SYSTEM_2-GRAM_odd_topic': 1, 'SYSTEM_2-GRAM_topic_.': 1, 'SYSTEM_2-GRAM_._pirates': 1, 'SYSTEM_2-GRAM_pirates_!': 1, 'SYSTEM_2-GRAM_!_i': 1, 'SYSTEM_2-GRAM_i_can': 1, 'SYSTEM_2-GRAM_can_even': 1, 'SYSTEM_2-GRAM_even_talk': 1, 'SYSTEM_2-GRAM_talk_like': 1, 'SYSTEM_2-GRAM_like_a': 1, 'SYSTEM_2-GRAM_a_pirate': 1, 'SYSTEM_2-GRAM_pirate_!': 1, 'SYSTEM_2-GRAM_!_arr': 1, 'SYSTEM_2-GRAM_arr_my': 1, 'SYSTEM_2-GRAM_my_matey': 1, 'SYSTEM_2-GRAM_matey_.': 1, 'SYSTEM_2-GRAM_._i': 2, 'SYSTEM_2-GRAM_i_know': 1, 'SYSTEM_2-GRAM_know_a': 1, 'SYSTEM_2-GRAM_a_lot': 1, 'SYSTEM_2-GRAM_lot_of': 1, 'SYSTEM_2-GRAM_of_history': 1, 'SYSTEM_2-GRAM_history_and': 1, 'SYSTEM_2-GRAM_and_odd': 1, 'SYSTEM_2-GRAM_odd_facts': 1, 'SYSTEM_2-GRAM_facts_about': 1, 'SYSTEM_2-GRAM_about_pirates': 2, 'SYSTEM_2-GRAM_pirates_.': 1, 'SYSTEM_2-GRAM_._we': 1, 'SYSTEM_2-GRAM_we_could': 1, 'SYSTEM_2-GRAM_could_talk': 1, 'SYSTEM_2-GRAM_talk_about': 1, 'SYSTEM_2-GRAM_pirates_,': 1, 'SYSTEM_2-GRAM_,_if': 1, 'SYSTEM_2-GRAM_if_you': 2, 'SYSTEM_2-GRAM_you_are': 1, 'SYSTEM_2-GRAM_are_interested': 1, 'SYSTEM_2-GRAM_interested_?': 1, 'SYSTEM_2-GRAM_?_i': 2, \"SYSTEM_2-GRAM_i_'ve\": 1, \"SYSTEM_2-GRAM_'ve_become\": 1, 'SYSTEM_2-GRAM_become_really': 1, 'SYSTEM_2-GRAM_really_interested': 1, 'SYSTEM_2-GRAM_interested_in': 2, 'SYSTEM_2-GRAM_in_food': 2, 'SYSTEM_2-GRAM_food_lately': 1, 'SYSTEM_2-GRAM_lately_!': 1, 'SYSTEM_2-GRAM_!_it': 1, \"SYSTEM_2-GRAM_it_'s\": 1, \"SYSTEM_2-GRAM_'s_really\": 1, 'SYSTEM_2-GRAM_really_cool': 1, 'SYSTEM_2-GRAM_cool_learning': 1, 'SYSTEM_2-GRAM_learning_about': 1, 'SYSTEM_2-GRAM_about_all': 1, 'SYSTEM_2-GRAM_all_the': 1, 'SYSTEM_2-GRAM_the_different': 1, 'SYSTEM_2-GRAM_different_cuisines': 1, 'SYSTEM_2-GRAM_cuisines_.': 1, \"SYSTEM_2-GRAM_i_'m\": 3, \"SYSTEM_2-GRAM_'m_curious\": 1, 'SYSTEM_2-GRAM_curious_,': 1, 'SYSTEM_2-GRAM_,_are': 1, 'SYSTEM_2-GRAM_are_you': 1, 'SYSTEM_2-GRAM_you_interested': 1, 'SYSTEM_2-GRAM_food_?': 1, \"SYSTEM_2-GRAM_'m_sorry\": 1, 'SYSTEM_2-GRAM_sorry_if': 1, 'SYSTEM_2-GRAM_if_i': 1, 'SYSTEM_2-GRAM_i_was': 1, 'SYSTEM_2-GRAM_was_somewhat': 1, 'SYSTEM_2-GRAM_somewhat_incoherent': 1, 'SYSTEM_2-GRAM_incoherent_,': 1, 'SYSTEM_2-GRAM_,_my': 1, 'SYSTEM_2-GRAM_my_conversation': 1, 'SYSTEM_2-GRAM_conversation_skills': 1, 'SYSTEM_2-GRAM_skills_are': 1, 'SYSTEM_2-GRAM_are_limited': 1, 'SYSTEM_2-GRAM_limited_for': 1, 'SYSTEM_2-GRAM_for_now': 1, 'SYSTEM_2-GRAM_now_,': 1, 'SYSTEM_2-GRAM_,_but': 1, 'SYSTEM_2-GRAM_but_i': 1, 'SYSTEM_2-GRAM_i_think': 1, 'SYSTEM_2-GRAM_think_i': 1, \"SYSTEM_2-GRAM_'m_getting\": 1, 'SYSTEM_2-GRAM_getting_there': 1, 'SYSTEM_2-GRAM_there_.': 1, 'SYSTEM_2-GRAM_._please': 1, 'SYSTEM_2-GRAM_please_say': 1, 'SYSTEM_2-GRAM_say_stop': 1, 'SYSTEM_2-GRAM_stop_if': 1, 'SYSTEM_2-GRAM_you_wish': 1, 'SYSTEM_2-GRAM_wish_to': 1, 'SYSTEM_2-GRAM_to_end': 1, 'SYSTEM_2-GRAM_end_the': 1, 'SYSTEM_2-GRAM_the_conversation': 1, 'SYSTEM_2-GRAM_conversation_.': 1}\n",
            "{'CONV_LENGTH': 87, 'USER_2-GRAM_no_alexa': 1, 'USER_2-GRAM_alexa_can': 1, 'USER_2-GRAM_can_we': 1, 'USER_2-GRAM_we_have': 1, 'USER_2-GRAM_have_a': 1, 'USER_2-GRAM_a_little': 1, 'USER_2-GRAM_little_break': 1, 'USER_2-GRAM_break_please': 1, 'USER_2-GRAM_please_stapp': 1, 'SYSTEM_2-GRAM_yeah_,': 1, 'SYSTEM_2-GRAM_,_i': 2, 'SYSTEM_2-GRAM_i_agree': 1, 'SYSTEM_2-GRAM_agree_,': 1, 'SYSTEM_2-GRAM_,_that': 1, \"SYSTEM_2-GRAM_that_'s\": 1, \"SYSTEM_2-GRAM_'s_really\": 1, 'SYSTEM_2-GRAM_really_fantastic': 1, 'SYSTEM_2-GRAM_fantastic_.': 1, 'SYSTEM_2-GRAM_._one': 1, 'SYSTEM_2-GRAM_one_animal': 1, 'SYSTEM_2-GRAM_animal_that': 1, 'SYSTEM_2-GRAM_that_i': 1, 'SYSTEM_2-GRAM_i_find': 1, 'SYSTEM_2-GRAM_find_super': 1, 'SYSTEM_2-GRAM_super_interesting': 1, 'SYSTEM_2-GRAM_interesting_is': 1, 'SYSTEM_2-GRAM_is_a': 1, 'SYSTEM_2-GRAM_a_common': 1, 'SYSTEM_2-GRAM_common_duiker': 1, 'SYSTEM_2-GRAM_duiker_!': 1, 'SYSTEM_2-GRAM_!_have': 1, 'SYSTEM_2-GRAM_have_you': 1, 'SYSTEM_2-GRAM_you_ever': 1, 'SYSTEM_2-GRAM_ever_seen': 1, 'SYSTEM_2-GRAM_seen_one': 1, 'SYSTEM_2-GRAM_one_?': 1, 'SYSTEM_2-GRAM_?_oh': 1, 'SYSTEM_2-GRAM_oh_,': 1, 'SYSTEM_2-GRAM_i_see': 1, 'SYSTEM_2-GRAM_see_.': 1, 'SYSTEM_2-GRAM_._they': 1, 'SYSTEM_2-GRAM_they_are': 1, 'SYSTEM_2-GRAM_are_pretty': 1, 'SYSTEM_2-GRAM_pretty_cool': 1, 'SYSTEM_2-GRAM_cool_.': 1, 'SYSTEM_2-GRAM_._next': 1, 'SYSTEM_2-GRAM_next_time': 1, 'SYSTEM_2-GRAM_time_you': 1, 'SYSTEM_2-GRAM_you_go': 1, 'SYSTEM_2-GRAM_go_to': 1, 'SYSTEM_2-GRAM_to_the': 2, 'SYSTEM_2-GRAM_the_zoo': 1, 'SYSTEM_2-GRAM_zoo_,': 1, 'SYSTEM_2-GRAM_,_try': 1, 'SYSTEM_2-GRAM_try_to': 1, 'SYSTEM_2-GRAM_to_check': 1, 'SYSTEM_2-GRAM_check_them': 1, 'SYSTEM_2-GRAM_them_out': 1, 'SYSTEM_2-GRAM_out_.': 1, 'SYSTEM_2-GRAM_._i': 1, \"SYSTEM_2-GRAM_i_'d\": 1, \"SYSTEM_2-GRAM_'d_love\": 1, 'SYSTEM_2-GRAM_love_to': 1, 'SYSTEM_2-GRAM_to_know': 1, 'SYSTEM_2-GRAM_know_what': 1, 'SYSTEM_2-GRAM_what_you': 1, 'SYSTEM_2-GRAM_you_think': 1, 'SYSTEM_2-GRAM_think_.': 1, 'SYSTEM_2-GRAM_._would': 1, 'SYSTEM_2-GRAM_would_you': 1, 'SYSTEM_2-GRAM_you_rather': 1, 'SYSTEM_2-GRAM_rather_go': 1, 'SYSTEM_2-GRAM_go_on': 1, 'SYSTEM_2-GRAM_on_a': 1, 'SYSTEM_2-GRAM_a_safari': 1, 'SYSTEM_2-GRAM_safari_drive': 1, 'SYSTEM_2-GRAM_drive_or': 1, 'SYSTEM_2-GRAM_or_to': 1, 'SYSTEM_2-GRAM_the_biggest': 1, 'SYSTEM_2-GRAM_biggest_zoo': 1, 'SYSTEM_2-GRAM_zoo_in': 1, 'SYSTEM_2-GRAM_in_the': 1, 'SYSTEM_2-GRAM_the_world': 1, 'SYSTEM_2-GRAM_world_?': 1, 'SYSTEM_2-GRAM_?_here': 1, \"SYSTEM_2-GRAM_here_'s\": 1, \"SYSTEM_2-GRAM_'s_something\": 1, 'SYSTEM_2-GRAM_something_amazing': 1, 'SYSTEM_2-GRAM_amazing_i': 1, 'SYSTEM_2-GRAM_i_read': 1, 'SYSTEM_2-GRAM_read_about': 1, 'SYSTEM_2-GRAM_about_tigers': 1, 'SYSTEM_2-GRAM_tigers_.': 1, 'SYSTEM_2-GRAM_._you': 1, 'SYSTEM_2-GRAM_you_might': 1, 'SYSTEM_2-GRAM_might_think': 1, 'SYSTEM_2-GRAM_think_it': 1, \"SYSTEM_2-GRAM_it_'s\": 1, \"SYSTEM_2-GRAM_'s_just\": 1, 'SYSTEM_2-GRAM_just_their': 1, 'SYSTEM_2-GRAM_their_fur': 1, 'SYSTEM_2-GRAM_fur_,': 1, 'SYSTEM_2-GRAM_,_but': 1, 'SYSTEM_2-GRAM_but_tigers': 1, 'SYSTEM_2-GRAM_tigers_have': 1, 'SYSTEM_2-GRAM_have_striped': 1, 'SYSTEM_2-GRAM_striped_skin': 1, 'SYSTEM_2-GRAM_skin_.': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZPcvBjqFvyp"
      },
      "source": [
        "# Binary Classification\n",
        "In this section, we create classifiers which distinguish between two of the rating groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAyrv2AEaNk0"
      },
      "source": [
        "def get_training_tuning_sets(train1, tune1, rating1, train2, tune2, rating2, min_length, shuffle=True, random_state=42):\n",
        "  \"\"\"\n",
        "  Compile and return the training and tuning datasets located at the given directories.\n",
        "\n",
        "  Params:\n",
        "    train1 : dataframe of the training data of first rating\n",
        "    tune1 : dataframe of tuning data of first rating\n",
        "    rating1 : The rating of the first files\n",
        "    train2 : dataframe of training data of second rating\n",
        "    tune2 : dataframe of tuning data of second rating\n",
        "    rating2 : The rating of the second files\n",
        "    shuffle : bool determining whether the resulting sets are shuffled before returning\n",
        "    random_state : int, only used if shuffle is True, allows for reproducible shuffling\n",
        "  Returns:\n",
        "    training_frame : pandas dataframe containing information compiled from the two\n",
        "      training files using information compilation and processing functions defined above\n",
        "    tuning_frame : pandas dataframe containing information compiled from the two\n",
        "      tuning files using information compilation and processing functions defined above\n",
        "  \"\"\"\n",
        "\n",
        "  # Compile info with rating for each dataframe\n",
        "  train1_compile = compile_info(train1, rating=rating1, min_length=min_length)\n",
        "  tune1_compile = compile_info(tune1, rating=rating1, min_length = min_length)\n",
        "  train2_compile = compile_info(train2, rating=rating2, min_length=min_length)\n",
        "  tune2_compile = compile_info(tune2, rating=rating2, min_length=min_length)\n",
        "\n",
        "  # Combine training dataframes and tuning dataframes\n",
        "  training_combo = pd.concat([train1_compile, train2_compile], ignore_index=True)\n",
        "  tuning_combo = pd.concat([tune1_compile, tune2_compile], ignore_index=True)\n",
        "\n",
        "  # Preprocess information (Slow, but saves time in feature extraction by doing all the hard work here)\n",
        "  tokenize_frame_texts(training_combo)\n",
        "  tokenize_frame_texts(tuning_combo)\n",
        "  sum_frame_tokens(training_combo)\n",
        "  sum_frame_tokens(tuning_combo)\n",
        "  get_liwc_scores(training_combo)\n",
        "  get_liwc_scores(tuning_combo)\n",
        "\n",
        "  # Optional shuffle\n",
        "  if shuffle:\n",
        "    training_combo = training_combo.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "    tuning_combo = tuning_combo.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "  return training_combo, tuning_combo"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph53Q-ZAs_4P"
      },
      "source": [
        "def get_training_tuning_sets_reg(train1, tune1, rating1, train2, tune2, rating2, train3, tune3, rating3, train4, tune4, rating4, train5, tune5, rating5, min_length, shuffle=True, random_state=42):\n",
        "  \"\"\"\n",
        "  Compile and return the training and tuning datasets located at the given directories.\n",
        "\n",
        "  Params:\n",
        "    train1 : dataframe of the training data of first rating\n",
        "    tune1 : dataframe of tuning data of first rating\n",
        "    rating1 : The rating of the first files\n",
        "    train2 : dataframe of training data of second rating\n",
        "    tune2 : dataframe of tuning data of second rating\n",
        "    rating2 : The rating of the second files\n",
        "    shuffle : bool determining whether the resulting sets are shuffled before returning\n",
        "    random_state : int, only used if shuffle is True, allows for reproducible shuffling\n",
        "  Returns:\n",
        "    training_frame : pandas dataframe containing information compiled from the two\n",
        "      training files using information compilation and processing functions defined above\n",
        "    tuning_frame : pandas dataframe containing information compiled from the two\n",
        "      tuning files using information compilation and processing functions defined above\n",
        "  \"\"\"\n",
        "\n",
        "  # Compile info with rating for each dataframe\n",
        "  train1_compile = compile_info(train1, rating=rating1, min_length=min_length)\n",
        "  tune1_compile = compile_info(tune1, rating=rating1, min_length = min_length)\n",
        "  train2_compile = compile_info(train2, rating=rating2, min_length=min_length)\n",
        "  tune2_compile = compile_info(tune2, rating=rating2, min_length=min_length)\n",
        "  train3_compile = compile_info(train3, rating=rating3, min_length=min_length)\n",
        "  tune3_compile = compile_info(tune3, rating=rating3, min_length = min_length)\n",
        "  train4_compile = compile_info(train4, rating=rating4, min_length=min_length)\n",
        "  tune4_compile = compile_info(tune4, rating=rating4, min_length = min_length)\n",
        "  train5_compile = compile_info(train5, rating=rating5, min_length=min_length)\n",
        "  tune5_compile = compile_info(tune5, rating=rating5, min_length = min_length)\n",
        "\n",
        "  # Combine training dataframes and tuning dataframes\n",
        "  training_combo = pd.concat([train1_compile, train2_compile, train3_compile, train4_compile, train5_compile], ignore_index=True)\n",
        "  tuning_combo = pd.concat([tune1_compile, tune2_compile, tune3_compile, tune4_compile, tune5_compile], ignore_index=True)\n",
        "\n",
        "  # Preprocess information (Slow, but saves time in feature extraction by doing all the hard work here)\n",
        "  tokenize_frame_texts(training_combo)\n",
        "  tokenize_frame_texts(tuning_combo)\n",
        "  sum_frame_tokens(training_combo)\n",
        "  sum_frame_tokens(tuning_combo)\n",
        "  get_liwc_scores(training_combo)\n",
        "  get_liwc_scores(tuning_combo)\n",
        "\n",
        "  # Optional shuffle\n",
        "  if shuffle:\n",
        "    training_combo = training_combo.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "    tuning_combo = tuning_combo.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "  return training_combo, tuning_combo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fftuaIksPYeY"
      },
      "source": [
        "train1 = open_file(DATA_DIR + \"/Training Data/rating-1-training-set-profanity-tagged.tsv\")\n",
        "tune1 = open_file(DATA_DIR + \"/Tuning Data/rating-1-tuning-set-profanity-tagged.tsv\")\n",
        "train2 = open_file(DATA_DIR + \"/Training Data/rating-5-training-set-profanity-tagged.tsv\")\n",
        "tune2 = open_file(DATA_DIR + \"/Tuning Data/rating-5-tuning-set-profanity-tagged.tsv\")\n",
        "train3 = open_file(DATA_DIR + \"/Training Data/rating-2-training-set-profanity-tagged.tsv\")\n",
        "tune3 = open_file(DATA_DIR + \"/Tuning Data/rating-2-tuning-set-profanity-tagged.tsv\")\n",
        "trainBad = combine(train1, train3)\n",
        "tuneBad = combine(tune1, tune3)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M1lAj5XjeXZ"
      },
      "source": [
        "training_set, tuning_set = get_training_tuning_sets(train1=trainBad,\n",
        "                                                    tune1=tuneBad, \n",
        "                                                    rating1=\"bad\", \n",
        "                                                    train2=train2, \n",
        "                                                    tune2=tune2, \n",
        "                                                    rating2=\"good\",\n",
        "                                                    min_length = 7, \n",
        "                                                    shuffle=True, \n",
        "                                                    random_state=42)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S3NY00iOrLlr",
        "outputId": "dd6b3cdb-032c-43a0-ad53-87e76b9275ed"
      },
      "source": [
        "print(training_set.shape, tuning_set.shape)\n",
        "training_set"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13331, 15) (1679, 15)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>conversation_length</th>\n",
              "      <th>user_text</th>\n",
              "      <th>system_text</th>\n",
              "      <th>topic_dist</th>\n",
              "      <th>rg_dist</th>\n",
              "      <th>profanities</th>\n",
              "      <th>midas</th>\n",
              "      <th>midas_dist</th>\n",
              "      <th>user_tokens</th>\n",
              "      <th>system_tokens</th>\n",
              "      <th>user_token_count</th>\n",
              "      <th>LIWC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1ba8bc33b275aeada15869c1e3805de85bd6d2cb4dca2b...</td>\n",
              "      <td>good</td>\n",
              "      <td>2021-06-07</td>\n",
              "      <td>59</td>\n",
              "      <td>[let's chat, good, caleb, yeah, yeah, no, go o...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. I hope ...</td>\n",
              "      <td>{'introduction': 18, 'movies': 24, 'neutral': ...</td>\n",
              "      <td>{'INTRODUCTION': 18, 'DM_GENERATOR': 1, 'CENTE...</td>\n",
              "      <td>1</td>\n",
              "      <td>[[], [comment], [statement], [back-channeling]...</td>\n",
              "      <td>{'comment': 2, 'statement': 8, 'back-channelin...</td>\n",
              "      <td>[let's, chat, good, caleb, yeah, yeah, go, vac...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>90</td>\n",
              "      <td>{'Word Count': 141, 'All Punctuation': 43.9716...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>124a3f8842221a21dc505129d9645c95bd19cfcd8f66b3...</td>\n",
              "      <td>good</td>\n",
              "      <td>2021-03-19</td>\n",
              "      <td>12</td>\n",
              "      <td>[talk to me, good, tanya, yes, repeat, i didn'...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 11}</td>\n",
              "      <td>{'INTRODUCTION': 8, 'RepeatGenerator': 3, 'GOO...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [back-channeling], [statement], [pos_answ...</td>\n",
              "      <td>{'back-channeling': 3, 'statement': 4, 'pos_an...</td>\n",
              "      <td>[talk, good, tanya, yes, repeat, wear, green, ...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>19</td>\n",
              "      <td>{'Word Count': 35, 'Social Processes': 2.85714...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>428e47f2eab42fc4ce68e596b4f670571c85a96157a5c5...</td>\n",
              "      <td>bad</td>\n",
              "      <td>2021-02-02</td>\n",
              "      <td>10</td>\n",
              "      <td>[talk to me, good, yes, nothing, restaurants, ...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 9, 'nutrition': 1}</td>\n",
              "      <td>{'INTRODUCTION': 9, 'DM_GENERATOR': 1}</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [back-channeling], [pos_answer], [other_a...</td>\n",
              "      <td>{'back-channeling': 1, 'pos_answer': 2, 'other...</td>\n",
              "      <td>[talk, good, yes, nothing, restaurants, amazin...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>16</td>\n",
              "      <td>{'Word Count': 28, 'Social Processes': 7.14285...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fcdadfd90c4439c26e04b84fac7195fdaf7340cfbae549...</td>\n",
              "      <td>bad</td>\n",
              "      <td>2021-03-28</td>\n",
              "      <td>10</td>\n",
              "      <td>[let's chat, good real good, steve, yes, yes, ...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How's i...</td>\n",
              "      <td>{'introduction': 10}</td>\n",
              "      <td>{'INTRODUCTION': 9, 'RepeatGenerator': 1}</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [comment, back-channeling, comment], [sta...</td>\n",
              "      <td>{'comment': 2, 'back-channeling': 2, 'statemen...</td>\n",
              "      <td>[let's, chat, good, real, good, steve, yes, ye...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>19</td>\n",
              "      <td>{'Word Count': 25, 'All Punctuation': 40.0, 'O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>09660507d5cc13c713e64c1e7f6285a33b3bfc0472242e...</td>\n",
              "      <td>good</td>\n",
              "      <td>2021-06-02</td>\n",
              "      <td>12</td>\n",
              "      <td>[let's chat, good, amelia, yes, yeah, no, uh-o...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. I hope ...</td>\n",
              "      <td>{'introduction': 12}</td>\n",
              "      <td>{'INTRODUCTION': 12}</td>\n",
              "      <td>1</td>\n",
              "      <td>[[], [comment], [statement], [pos_answer], [po...</td>\n",
              "      <td>{'comment': 1, 'statement': 5, 'pos_answer': 2...</td>\n",
              "      <td>[let's, chat, good, amelia, yes, yeah, uh-oh, ...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>13</td>\n",
              "      <td>{'Word Count': 24, 'All Punctuation': 54.16666...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13326</th>\n",
              "      <td>0eaf5be3b8f18a5b8301ba016976b0ab547cffa68a5331...</td>\n",
              "      <td>good</td>\n",
              "      <td>2021-03-19</td>\n",
              "      <td>20</td>\n",
              "      <td>[let's chat, it's going great this what are ne...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How's i...</td>\n",
              "      <td>{'introduction': 12, 'harry_potter': 6, 'menu_...</td>\n",
              "      <td>{'INTRODUCTION': 12, 'DM_GENERATOR': 1, 'HARRY...</td>\n",
              "      <td>1</td>\n",
              "      <td>[[], [statement, comment, abandon, open_questi...</td>\n",
              "      <td>{'statement': 11, 'comment': 5, 'abandon': 3, ...</td>\n",
              "      <td>[let's, chat, going, great, new, students, com...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>73</td>\n",
              "      <td>{'Word Count': 117, 'All Punctuation': 17.0940...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13327</th>\n",
              "      <td>f4e0ff68bc826bdf2e25d5109a3088c91359782da9e6d7...</td>\n",
              "      <td>good</td>\n",
              "      <td>2021-04-14</td>\n",
              "      <td>15</td>\n",
              "      <td>[let's chat, good how's it going to you, thank...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How's i...</td>\n",
              "      <td>{'introduction': 13, 'neutral': 2}</td>\n",
              "      <td>{'INTRODUCTION': 11, 'SB_INDEX': 3, 'RepeatGen...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [open_question_factual, back-channeling, ...</td>\n",
              "      <td>{'open_question_factual': 3, 'back-channeling'...</td>\n",
              "      <td>[let's, chat, good, how's, going, thanks, cath...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>35</td>\n",
              "      <td>{'Word Count': 59, 'All Punctuation': 35.59322...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13328</th>\n",
              "      <td>f8cc0427f2aa6b1ab7099d097e65c673071578dd58b476...</td>\n",
              "      <td>good</td>\n",
              "      <td>2021-04-06</td>\n",
              "      <td>10</td>\n",
              "      <td>[let's chat, i'm great, big fat so, no i can't...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 8, 'nature': 1, 'video_games'...</td>\n",
              "      <td>{'INTRODUCTION': 8, 'DM_GENERATOR': 1, 'VIDEO_...</td>\n",
              "      <td>2</td>\n",
              "      <td>[[], [statement], [abandon], [statement, neg_a...</td>\n",
              "      <td>{'statement': 5, 'abandon': 1, 'neg_answer': 1...</td>\n",
              "      <td>[let's, chat, i'm, great, big, fat, can't, wal...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>27</td>\n",
              "      <td>{'Word Count': 44, 'All Punctuation': 22.72727...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13329</th>\n",
              "      <td>3ba1ed533d34bd2f986a7f1d2b83ff3776f9bf606df972...</td>\n",
              "      <td>bad</td>\n",
              "      <td>2021-03-18</td>\n",
              "      <td>31</td>\n",
              "      <td>[have a conversation, i'm okay how are you, ne...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How are...</td>\n",
              "      <td>{'introduction': 11, 'nature': 9, 'food': 8, '...</td>\n",
              "      <td>{'INTRODUCTION': 10, 'RepeatGenerator': 2, 'DM...</td>\n",
              "      <td>1</td>\n",
              "      <td>[[], [open_question_factual, back-channeling, ...</td>\n",
              "      <td>{'open_question_factual': 6, 'back-channeling'...</td>\n",
              "      <td>[conversation, i'm, okay, next, wanna, know, u...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>56</td>\n",
              "      <td>{'Word Count': 118, 'Six Letter Words': 12.711...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13330</th>\n",
              "      <td>3e177d6c1f13e33820e8637f157176e697516dd49bae47...</td>\n",
              "      <td>good</td>\n",
              "      <td>2021-05-08</td>\n",
              "      <td>48</td>\n",
              "      <td>[can you talk to me, good, reading, no, yeah w...</td>\n",
              "      <td>[Hi, this is an Alexa Prize Socialbot. How's i...</td>\n",
              "      <td>{'introduction': 16, 'hobbies': 5, 'video_game...</td>\n",
              "      <td>{'INTRODUCTION': 16, 'DM_GENERATOR': 1, 'HOBBI...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[], [back-channeling], [statement], [neg_answ...</td>\n",
              "      <td>{'back-channeling': 6, 'statement': 27, 'neg_a...</td>\n",
              "      <td>[talk, good, reading, yeah, what's, movie, the...</td>\n",
              "      <td>[Hi, ,, this, is, an, Alexa, Prize, Socialbot,...</td>\n",
              "      <td>143</td>\n",
              "      <td>{'Word Count': 271, 'Social Processes': 3.6900...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13331 rows Ã— 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         conversation_id  ...                                               LIWC\n",
              "0      1ba8bc33b275aeada15869c1e3805de85bd6d2cb4dca2b...  ...  {'Word Count': 141, 'All Punctuation': 43.9716...\n",
              "1      124a3f8842221a21dc505129d9645c95bd19cfcd8f66b3...  ...  {'Word Count': 35, 'Social Processes': 2.85714...\n",
              "2      428e47f2eab42fc4ce68e596b4f670571c85a96157a5c5...  ...  {'Word Count': 28, 'Social Processes': 7.14285...\n",
              "3      fcdadfd90c4439c26e04b84fac7195fdaf7340cfbae549...  ...  {'Word Count': 25, 'All Punctuation': 40.0, 'O...\n",
              "4      09660507d5cc13c713e64c1e7f6285a33b3bfc0472242e...  ...  {'Word Count': 24, 'All Punctuation': 54.16666...\n",
              "...                                                  ...  ...                                                ...\n",
              "13326  0eaf5be3b8f18a5b8301ba016976b0ab547cffa68a5331...  ...  {'Word Count': 117, 'All Punctuation': 17.0940...\n",
              "13327  f4e0ff68bc826bdf2e25d5109a3088c91359782da9e6d7...  ...  {'Word Count': 59, 'All Punctuation': 35.59322...\n",
              "13328  f8cc0427f2aa6b1ab7099d097e65c673071578dd58b476...  ...  {'Word Count': 44, 'All Punctuation': 22.72727...\n",
              "13329  3ba1ed533d34bd2f986a7f1d2b83ff3776f9bf606df972...  ...  {'Word Count': 118, 'Six Letter Words': 12.711...\n",
              "13330  3e177d6c1f13e33820e8637f157176e697516dd49bae47...  ...  {'Word Count': 271, 'Social Processes': 3.6900...\n",
              "\n",
              "[13331 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YywcFv55pA5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64bf693f-f731-4388-c8b9-5e54cbcba9ea"
      },
      "source": [
        "# Baseline accuracy\n",
        "print(\"Baseline Accuracy: {}\".format(np.sum(tuning_set['rating']=='good')/len(tuning_set)))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline Accuracy: 0.6521739130434783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6TR6jw1U68Y"
      },
      "source": [
        "Now we can extract features from the training set, and train some algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9tECA1ZU4vC",
        "outputId": "583a6d88-9a25-445b-9788-ec86d646dda3"
      },
      "source": [
        "# Extract features and pair with rating\n",
        "\n",
        "#feature_vectors = extract_features(training_set, user_unigrams=False, user_bigrams= True, profane_count= False, USER_WORD_COUNT=False, topic_counts=False, user_trigrams=True, CONV_LENGTH=True)\n",
        "#feature_vectors = extract_features(training_set, last_n_turns=True, n_turns=3, CONV_LENGTH=True, user_bigrams=True, system_bigrams=True)\n",
        "#topic_counts=True, desired_topics=['none'], include_topic_total=True, topic_metrics=True\n",
        "#rg_counts=True, desired_rgs=['redquestion', 'RepeatGenerator'], midas_counts=True, desired_midas=['neg_answer', 'complaint'], user_unigrams=True\n",
        "\"\"\"feature_vectors = extract_features(training_set, system_utter_metrics=True, system_metrics_to_use=['mean', 'std', 'max', 'min'], user_utter_metrics=True, user_metrics_to_use=['mean', 'std', 'max', 'min'], \n",
        "                                   rg_counts=True, desired_rgs=['RepeatGenerator', 'redquestion'], topic_counts=True, include_topic_total=True, topic_metrics=True, midas_counts=True, desired_midas=['neg_answer', 'complaint'],\n",
        "                                   CONV_LENGTH=True, profane_count=True)\"\"\"\n",
        "feature_vectors = extract_features(training_set, user_unigrams=True, user_bigrams=False, user_trigrams=False, profane_count=False, topic_freq=False, CONV_LENGTH=False, liwc_scores=False, midas_freq=False)\n",
        "training_features = list(zip(feature_vectors, training_set['rating']))\n",
        "print(training_features[0])\n",
        "print(training_features[1])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "({\"USER_1-GRAM_let's\": 2, 'USER_1-GRAM_chat': 1, 'USER_1-GRAM_good': 1, 'USER_1-GRAM_caleb': 1, 'USER_1-GRAM_yeah': 3, 'USER_1-GRAM_go': 2, 'USER_1-GRAM_vacation': 1, 'USER_1-GRAM_definitely': 2, 'USER_1-GRAM_wearing': 1, 'USER_1-GRAM_masks': 1, 'USER_1-GRAM_movie': 2, 'USER_1-GRAM_yes': 3, 'USER_1-GRAM_probably': 3, 'USER_1-GRAM_pet': 1, \"USER_1-GRAM_i'm\": 1, 'USER_1-GRAM_new': 2, 'USER_1-GRAM_york': 1, 'USER_1-GRAM_need': 1, 'USER_1-GRAM_repeat': 1, 'USER_1-GRAM_may': 1, 'USER_1-GRAM_school': 2, 'USER_1-GRAM_nah': 1, 'USER_1-GRAM_maybe': 1, 'USER_1-GRAM_try': 1, 'USER_1-GRAM_stuff': 1, \"USER_1-GRAM_what's\": 1, 'USER_1-GRAM_favorite': 1, 'USER_1-GRAM_color': 1, 'USER_1-GRAM_wyatt': 1, 'USER_1-GRAM_wow': 2, 'USER_1-GRAM_hmm': 3, 'USER_1-GRAM_ha-ha': 1, 'USER_1-GRAM_seen': 2, 'USER_1-GRAM_high': 1, 'USER_1-GRAM_musical': 1, 'USER_1-GRAM_two': 1, 'USER_1-GRAM_love': 1, 'USER_1-GRAM_knew': 1, 'USER_1-GRAM_today': 1, 'USER_1-GRAM_aye': 2, 'USER_1-GRAM_ooh': 1, 'USER_1-GRAM_indian': 1, 'USER_1-GRAM_loved': 1, 'USER_1-GRAM_think': 1, 'USER_1-GRAM_scariest': 1, 'USER_1-GRAM_heard': 1, 'USER_1-GRAM_rain': 1, 'USER_1-GRAM_lost': 1, 'USER_1-GRAM_dragon': 1, 'USER_1-GRAM_know': 1, 'USER_1-GRAM_alexa': 1, 'USER_1-GRAM_talk': 1, 'USER_1-GRAM_different': 1, 'USER_1-GRAM_thing': 1, 'USER_1-GRAM_food': 1, 'USER_1-GRAM_mouth': 1, 'USER_1-GRAM_really': 1, 'USER_1-GRAM_monkey': 1, 'USER_1-GRAM_order': 1, 'USER_1-GRAM_last': 1, 'USER_1-GRAM_pizza': 1, 'USER_1-GRAM_tv': 1, 'USER_1-GRAM_time': 1, 'USER_1-GRAM_pow': 1, \"USER_1-GRAM_'s\": 1, 'USER_1-GRAM_conversation': 2, 'USER_1-GRAM_pause': 1, 'USER_1-GRAM_t.': 1, 'USER_1-GRAM_rex': 1, 'USER_1-GRAM_stapp': 1}, 'good')\n",
            "({'USER_1-GRAM_talk': 1, 'USER_1-GRAM_good': 1, 'USER_1-GRAM_tanya': 1, 'USER_1-GRAM_yes': 1, 'USER_1-GRAM_repeat': 1, 'USER_1-GRAM_wear': 1, 'USER_1-GRAM_green': 1, 'USER_1-GRAM_want': 1, 'USER_1-GRAM_home': 1, 'USER_1-GRAM_let': 1, \"USER_1-GRAM_let's\": 1, 'USER_1-GRAM_go': 2, 'USER_1-GRAM_wanna': 1, 'USER_1-GRAM_fiji': 1, 'USER_1-GRAM_okay': 1, 'USER_1-GRAM_could': 1, 'USER_1-GRAM_quiet': 1, 'USER_1-GRAM_stop': 1}, 'good')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl3ZAnzeWQPT"
      },
      "source": [
        "## Train a Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG-T_2xjTut_"
      },
      "source": [
        "# Here we train a model on the feature list we extracted\n",
        "# NaiveBayesClassifer from nltk\n",
        "#classifier = NaiveBayesClassifier.train(training_features)\n",
        "# sklearn BernoulliNB classifier\n",
        "#classifier = SklearnClassifier(BernoulliNB()).train(training_features)\n",
        "# Decision tree\n",
        "#classifier = SklearnClassifier(DecisionTreeClassifier()).train(training_features)\n",
        "# SVM\n",
        "#classifier = SklearnClassifier(SVC()).train(training_features)\n",
        "# MLP\n",
        "#classifier = SklearnClassifier(MLPClassifier(max_iter=400)).train(training_features)\n",
        "# Linear Regression\n",
        "classifier = SklearnClassifier(linear_model.LinearRegression()).train(training_features)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km0z87RnUD_X"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_CW8HckUFrx"
      },
      "source": [
        "from numpy.core.numeric import False_\n",
        "# Function to get accuracy, precision, recall, f_measure, and confusion matrix\n",
        "def evaluate_model(model, data, mistakes=False, last_n_turns = False, n_turns = 0, CONV_LENGTH = False, USER_WORD_COUNT = False, user_unigrams = False, user_bigrams = False, user_trigrams = False, system_unigrams = False, \n",
        "                   system_bigrams = False, system_trigrams = False, profane_count = False, topic_counts = False, topic_freq=False, desired_topics=None, include_topic_total=False, topic_metrics=False, topic_metrics_to_use=[\"mean\", \"std\", \"max\", \"min\"],\n",
        "                   midas_counts=False, midas_freq=False, desired_midas=None, liwc_scores=False, rg_counts=False, rg_freq=False, desired_rgs=None, system_utter_metrics=False, system_metrics_to_use=[\"mean\", \"std\", \"max\", \"min\"], \n",
        "                   user_utter_metrics=False, user_metrics_to_use=[\"mean\", \"std\", \"max\", \"min\"]):\n",
        "  # If mistakes==True, return the indices of the reviews that were misclassified\n",
        "  # Get feature vectors for the data\n",
        "  test_vectors = extract_features(data, last_n_turns=last_n_turns, n_turns=n_turns, CONV_LENGTH = CONV_LENGTH, USER_WORD_COUNT = USER_WORD_COUNT, user_unigrams = user_unigrams, user_bigrams = user_bigrams, \n",
        "                                  user_trigrams = user_trigrams, system_unigrams = system_unigrams, system_bigrams = system_bigrams, system_trigrams = system_trigrams, topic_counts=topic_counts, topic_freq=topic_freq, \n",
        "                                  desired_topics=desired_topics, include_topic_total=include_topic_total, topic_metrics=topic_metrics, topic_metrics_to_use=topic_metrics_to_use, midas_counts=midas_counts, \n",
        "                                  midas_freq=midas_freq, desired_midas=desired_midas, liwc_scores=liwc_scores, rg_counts=rg_counts, rg_freq=rg_freq, desired_rgs=desired_rgs, system_utter_metrics=system_utter_metrics,\n",
        "                                  system_metrics_to_use=system_metrics_to_use, user_utter_metrics=user_utter_metrics, user_metrics_to_use=user_metrics_to_use)\n",
        "  test_labels = list(data['rating'])\n",
        "  # Make predictions\n",
        "  predictions = model.classify_many(test_vectors)\n",
        "  # Get the misclassification indices\n",
        "  mistake_indices = None\n",
        "  if mistakes:\n",
        "    mistake_indices = []\n",
        "    for i in range(len(predictions)):\n",
        "      if not predictions[i] == test_labels[i]:\n",
        "        mistake_indices.append(i)\n",
        "\n",
        "  matrix = confusion_matrix(test_labels, predictions)\n",
        "  accuracy = round(nltk.classify.accuracy(model, list(zip(test_vectors, test_labels))), 3)\n",
        "  TP = matrix[1, 1]\n",
        "  TN = matrix[0, 0]\n",
        "  FP = matrix[0, 1]\n",
        "  FN = matrix[1, 0]\n",
        "  TPR = TP/np.sum(matrix[1])\n",
        "  TNR = TN/np.sum(matrix[0])\n",
        "  balanced_acc = round((TPR+TNR)/2, 3)\n",
        "  nltk_matrix = ConfusionMatrix(test_labels, predictions)\n",
        "  precision = round(TP/(TP+FP), 3)\n",
        "  recall = round(TP/(TP+FN), 3)\n",
        "  f_score = round((2*precision*recall)/(precision + recall), 3)\n",
        "  most_informative = []\n",
        "  if isinstance(model, NaiveBayesClassifier):\n",
        "    most_informative = model.most_informative_features(100)\n",
        "  return accuracy, balanced_acc, precision, recall, f_score, matrix, nltk_matrix, mistake_indices, most_informative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlbHHFr0Vmpn"
      },
      "source": [
        "# Evaluate the trained model on the tuning set\n",
        "# Note: Balanced Accuracy is a way of measuring the accuracy while taking into account\n",
        "# an imbalance in the number of samples in each category\n",
        "#acc, balanced_acc, prec, recall, f_score, matrix, nltk_matrix, mistakes, most_informative = evaluate_model(classifier, tuning_set, user_unigrams=False, user_bigrams = True, profane_count= True, USER_WORD_COUNT=False, topic_counts=False, user_trigrams= True, CONV_LENGTH = True)\n",
        "#acc, balanced_acc, prec, recall, f_score, matrix, nltk_matrix, mistakes, most_informative = evaluate_model(classifier, tuning_set, user_unigrams=True, profane_count=False, topic_freq=False, user_bigrams=False, user_trigrams=False, CONV_LENGTH=False, liwc_scores=False, midas_freq=False)\n",
        "acc, balanced_acc, prec, recall, f_score, matrix, nltk_matrix, mistakes, most_informative = evaluate_model(classifier, tuning_set, user_unigrams=True, user_bigrams=True, user_trigrams=True, midas_freq=True)\n",
        "print(\"Accuracy: {}\".format(acc))\n",
        "print(\"Balanced Accuracy: {}\".format(balanced_acc))\n",
        "print(\"Precision: {}\".format(prec))\n",
        "print(\"Recall: {}\".format(recall))\n",
        "print(\"f_measure: {}\".format(f_score))\n",
        "print(\"Confusion matrix:\\n{!s}\".format(nltk_matrix))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwL-d045iT33"
      },
      "source": [
        "print(most_informative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONo4DJPrpYsn"
      },
      "source": [
        "classifier.show_most_informative_features(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8fyKLX5WmaR"
      },
      "source": [
        "# Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsmmdYvyR_sQ"
      },
      "source": [
        "# Run feature selection with most informative features from Naive Bayes\n",
        "def run_feature_selection(train_df, test_df, model_obj, start, end, step, last_n_turns = False, n_turns = 0, CONV_LENGTH = False, USER_WORD_COUNT = False, user_unigrams = False, user_bigrams = False, user_trigrams = False, \n",
        "                          system_unigrams = False, system_bigrams = False, system_trigrams = False, profane_count = False, topic_counts = False, topic_freq=False, desired_topics=None, include_topic_total=False, \n",
        "                          topic_metrics=False, topic_metrics_to_use=[\"mean\", \"std\", \"max\", \"min\"], midas_counts=False, midas_freq=False, desired_midas=None, liwc_scores=False, rg_counts=False, rg_freq=False, \n",
        "                          desired_rgs=None, system_utter_metrics=False, system_metrics_to_use=[\"mean\", \"std\", \"max\", \"min\"], user_utter_metrics=False, user_metrics_to_use=[\"mean\", \"std\", \"max\", \"min\"]):\n",
        "  # Run classification using all given features, then take from that\n",
        "  # different sized subsets to train on subsequently.\n",
        "  # Starting with number start, then in steps of size step, until end\n",
        "  feature_vectors = extract_features(train_df, last_n_turns=last_n_turns, n_turns=n_turns, CONV_LENGTH = CONV_LENGTH, USER_WORD_COUNT = USER_WORD_COUNT, user_unigrams = user_unigrams, user_bigrams = user_bigrams, \n",
        "                                  user_trigrams = user_trigrams, system_unigrams = system_unigrams, system_bigrams = system_bigrams, system_trigrams = system_trigrams, topic_counts=topic_counts, topic_freq=topic_freq, \n",
        "                                  desired_topics=desired_topics, include_topic_total=include_topic_total, topic_metrics=topic_metrics, topic_metrics_to_use=topic_metrics_to_use, midas_counts=midas_counts, \n",
        "                                  midas_freq=midas_freq, desired_midas=desired_midas, liwc_scores=liwc_scores, rg_counts=rg_counts, rg_freq=rg_freq, desired_rgs=desired_rgs, system_utter_metrics=system_utter_metrics,\n",
        "                                  system_metrics_to_use=system_metrics_to_use, user_utter_metrics=user_utter_metrics, user_metrics_to_use=user_metrics_to_use)\n",
        "  #test_features = extract_features(test_df)\n",
        "  training_features = list(zip(feature_vectors, train_df['rating']))\n",
        "  model = NaiveBayesClassifier.train(training_features)\n",
        "  informative_features = [feat for feat, val in model.most_informative_features(end)]\n",
        "  print(len(informative_features))\n",
        "  \n",
        "  best = (-1*np.inf, 0, None)\n",
        "  for i in range(start, end, step):\n",
        "    # For each subset of the informative features, create a new feature set\n",
        "    # from the total number of features extracted where only features in that\n",
        "    # subset appear.\n",
        "    subset = set(informative_features[:i])\n",
        "    new_features = []\n",
        "    for vector, label in training_features:\n",
        "      new_vector = {}\n",
        "      for key in vector:\n",
        "        if key in subset:\n",
        "          new_vector[key] = vector[key]\n",
        "      new_features.append((new_vector, label))\n",
        "    # We now have a new set of features to train on\n",
        "    model = model_obj.train(new_features)\n",
        "    accuracy, balanced_acc, precision, recall, f_measure, matrix, nltk_matrix, mistakes, informative = evaluate_model(model, test_df, mistakes=False, last_n_turns=last_n_turns, n_turns=n_turns, \n",
        "                                                                                                                      CONV_LENGTH = CONV_LENGTH, USER_WORD_COUNT = USER_WORD_COUNT, \n",
        "                                                                                                                      user_unigrams = user_unigrams, user_bigrams = user_bigrams, user_trigrams = user_trigrams, \n",
        "                                                                                                                      system_unigrams = system_unigrams, system_bigrams = system_bigrams, \n",
        "                                                                                                                      system_trigrams = system_trigrams, topic_counts=topic_counts, topic_freq=topic_freq, \n",
        "                                                                                                                      desired_topics=desired_topics, include_topic_total=include_topic_total, \n",
        "                                                                                                                      topic_metrics=topic_metrics, topic_metrics_to_use=topic_metrics_to_use, \n",
        "                                                                                                                      midas_counts=midas_counts, midas_freq=midas_freq, desired_midas=desired_midas, \n",
        "                                                                                                                      liwc_scores=liwc_scores, rg_counts=rg_counts, rg_freq=rg_freq, \n",
        "                                                                                                                      desired_rgs=desired_rgs, system_utter_metrics=system_utter_metrics, \n",
        "                                                                                                                      system_metrics_to_use=system_metrics_to_use, user_utter_metrics=user_utter_metrics, \n",
        "                                                                                                                      user_metrics_to_use=user_metrics_to_use)\n",
        "    print(\"Accuracy {} and f_measure {} with {} features.\".format(accuracy, f_measure, i))\n",
        "    if f_measure > best[0]:\n",
        "      best = (f_measure, i, new_features)\n",
        "  # Return the best accuracy and the number of features associated and the features themselves\n",
        "  return best\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN00rYSsTSM9"
      },
      "source": [
        "# Naive Bayes\n",
        "#best_acc, best_num, best_features = run_feature_selection(training_set, tuning_set, NaiveBayesClassifier, 125000, 135000, 1000, user_unigrams=True, user_bigrams=True, user_trigrams=True, midas_freq=True)\n",
        "#best_acc, best_num, best_features = run_feature_selection(training_set, tuning_set, NaiveBayesClassifier, 20, 200, 20, user_unigrams=True)\n",
        "#print(best_acc, best_num)\n",
        "#best_model = NaiveBayesClassifier.train(best_features)\n",
        "#NBModel = best_model\n",
        "\n",
        "# SVM\n",
        "best_acc, best_num, best_features = run_feature_selection(training_set, tuning_set, SklearnClassifier(SVC()), 1000, 100000, 10000, user_unigrams=True, user_bigrams=True, user_trigrams=True, midas_freq=True)\n",
        "#best_acc, best_num, best_features = run_feature_selection(training_set, tuning_set, SklearnClassifier(SVC()), 20, 200, 20, user_unigrams=True)\n",
        "#print(best_acc, best_num)\n",
        "best_model = SklearnClassifier(SVC()).train(best_features)\n",
        "SVMModel = best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x0N7tMNYpJ2"
      },
      "source": [
        "acc, balanced_acc, prec, recall, f_score, matrix, nltk_matrix, mistakes, informative = evaluate_model(best_model, tuning_set, mistakes=False, user_unigrams=True, user_bigrams=True, user_trigrams=True, midas_freq=True)\n",
        "print(\"Accuracy: {}\".format(acc))\n",
        "print(\"Precision: {}\".format(prec))\n",
        "print(\"Recall: {}\".format(recall))\n",
        "print(\"f_measure: {}\".format(f_score))\n",
        "print(\"Confusion matrix:\\n{!s}\".format(nltk_matrix))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txEzrWhdg6ZY"
      },
      "source": [
        "## Adding experiments to a file\n",
        "This is a csv file containing results from experiments for binary classification. As one performs experiments, one can add rows to the binary_experiments dataframe containing information about the experiment. Then, when one is ready, one can update the csv file with the experiments contained in the dataframe.\\\n",
        "Note: Implement a way of adding notes to each experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmyV5-7LpnSM"
      },
      "source": [
        "# Place filepath of folder containing experiment files here\n",
        "TEMP_DIR = '/content/drive/MyDrive/Research/WalkerResearch/Experiments'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z-vsujdl0fw"
      },
      "source": [
        "# Create binary_experiments frame from first experiment\n",
        "desc = \"Baseline\"\n",
        "binary_experiments = pd.DataFrame([[desc, matrix[1, 1], matrix[0, 1], matrix[0, 0], matrix[1, 0], acc, balanced_acc, recall, prec, f_score, most_informative]], \n",
        "                                  columns=[\"Description\", \"TP\", \"FP\", \"TN\", \"FN\", \"Accuracy\", \"Balanced Accuracy\", \"Recall\", \"Precision\", \"F Measure\", \"Most informative\"])\n",
        "binary_experiments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYXDOoHMmLgy"
      },
      "source": [
        "# Update frame with new experiment\n",
        "desc = \"user unigrams(bins of 3), user bigrams(bins of 3), midas freq, user trigrams(bins of 3)\"\n",
        "binary_experiments = binary_experiments.append(pd.DataFrame([[desc, matrix[1, 1], matrix[0, 1], matrix[0, 0], matrix[1, 0], acc, balanced_acc, recall, prec, f_score, most_informative]], columns=[\"Description\", \"TP\", \"FP\", \"TN\", \"FN\", \"Accuracy\", \"Balanced Accuracy\", \"Recall\", \"Precision\", \"F Measure\", \"Most informative\"]), ignore_index=True)\n",
        "binary_experiments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ4FANnLg8RE"
      },
      "source": [
        "def update_experiment_file(filepath, experiments):\n",
        "  with open(filepath, 'a+') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
        "    csv_file.seek(0)\n",
        "    # If empty file, add title row\n",
        "    if not csv_file.read(1):\n",
        "      csv_writer.writerow([\"Description\", \"TP\", \"FP\", \"TN\", \"FN\", \"Accuracy\", \"Balanced Accuracy\", \"Recall\", \"Precision\", \"F Measure\"])\n",
        "    csv_file.seek(0)\n",
        "    # Append dataframe rows to file\n",
        "    for row in experiments.iterrows():\n",
        "      csv_writer.writerow(list(row[1]))\n",
        "\n",
        "def print_experiment_file(filepath):\n",
        "  with open(filepath, 'r') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    csv_file.seek(0)\n",
        "    if not csv_file.read(1):\n",
        "      print(\"Empty File\")\n",
        "    csv_file.seek(0)\n",
        "    index = 0\n",
        "    for row in csv_reader:\n",
        "      if index==0:\n",
        "        print(row)\n",
        "      else:\n",
        "        print(\"Description: \" + row[0])\n",
        "        print(\"\\t\" + str(row[1:]))\n",
        "      index += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IVjphCyqSm8"
      },
      "source": [
        "# Update file\n",
        "update_experiment_file(\"/content/drive/Shareddrives/Alexa Prize 4 (2020 21)/Data/Rating Analysis/Rating-wise grouped conversations/Experiments/BinaryExperimentsSVM_RatingBadRating5_min7.csv\", binary_experiments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eWqnHXisWzN"
      },
      "source": [
        "# Print file\n",
        "print_experiment_file(\"/content/drive/Shareddrives/Alexa Prize 4 (2020 21)/Data/Rating Analysis/Rating-wise grouped conversations/Experiments/BinaryExperimentsSVM_RatingBadRating5_min7.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eWxoArSGCWP"
      },
      "source": [
        "# Looking at Ngrams\n",
        "We calculate ngrams across all files and look at the distribution. The idea is to remove ngrams which have a very low occurrence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HomNfpWGO0_"
      },
      "source": [
        "all_files = ['rating-1-logs-2021-01-01-2021-06-16-min-len-3.tsv',\n",
        "            'rating-2-logs-2021-01-01-2021-06-16-min-len-3.tsv',\n",
        "            'rating-3-logs-2021-01-01-2021-06-16-min-len-3.tsv',\n",
        "            'rating-4-logs-2021-01-01-2021-06-16-min-len-3.tsv',\n",
        "            'rating-5-logs-2021-01-01-2021-06-16-min-len-3.tsv']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdb66C2boeF4"
      },
      "source": [
        "# Extract and compile text in each conversation\n",
        "def compile_texts(dataframe, rating, min_length):\n",
        "  \"\"\"\n",
        "  Iterate through the dataframe and compile information about each conversation.\n",
        "  This information includes:\n",
        "    The conversation ID \n",
        "    The length of the conversation\n",
        "    The date at which the conversation took place\n",
        "    The user utterances\n",
        "    The system utterances\n",
        "  \n",
        "  Params:\n",
        "    dataframe : pandas DataFrame of a logfile\n",
        "    rating : int, the rating of the conversations which appear in the dataframe\n",
        "    min_length : int, the minimum number of turns allowed\n",
        "  Returns:\n",
        "    A new pandas DataFrame containing the above information. Each row is one conversation.\n",
        "  \"\"\"\n",
        "  # compiled_info is a list of rows, each in the form of a list\n",
        "  compiled_info = []\n",
        "  # Iterate through the dataframe, extracting data about conversations\n",
        "  # and adding it to the compiled_info list\n",
        "  current_conv_id = dataframe['conversation_id'][0]\n",
        "  # Objects to hold information about the current conversation\n",
        "  user_text = []\n",
        "  system_text = []\n",
        "  conv_len = 0\n",
        "  for i in range(len(dataframe)):\n",
        "    # If the conversation id changes, we have a new conversation beginning\n",
        "    if not dataframe['conversation_id'][i] == current_conv_id:\n",
        "      # If conversation is long enough,\n",
        "      # Add info to compiled_info list\n",
        "      if conv_len >= min_length:\n",
        "        compiled_info.append([current_conv_id, rating, dataframe['date'][i-1], conv_len, user_text, system_text])\n",
        "      # Reset the conversation\n",
        "      user_text = []\n",
        "      system_text = []\n",
        "      conv_len = 0\n",
        "      # Update current conversation id\n",
        "      current_conv_id = dataframe['conversation_id'][i]\n",
        "    # Compile info for each row in a conversation\n",
        "    # If we have a non-terminal row, add one to the conversation length\n",
        "    if not np.isnan(dataframe['turn_count'][i]):\n",
        "      conv_len += 1\n",
        "    # If the user had a valid utterance, append it\n",
        "    if type(dataframe['text'][i]) is str:\n",
        "      user_text.append(dataframe['text'][i])\n",
        "    # If the system had a valid utterance, append the extracted text\n",
        "    system_utter = dataframe['response'][i]\n",
        "    if type(system_utter) is str:\n",
        "      system_text.append(extract_system_text(system_utter))\n",
        "    #else: The box is empty (end of a conversation)\n",
        "  # Add info for last conversation (current id doesn't change at the end of the file, but we still want the conversation info)\n",
        "  if conv_len >= min_length:\n",
        "    compiled_info.append([current_conv_id, rating, dataframe['date'][len(dataframe)-1], conv_len, user_text, system_text])\n",
        "\n",
        "  # Construct new dataframe from the info (each list in compiled_info becomes a row)\n",
        "  new_frame = pd.DataFrame(compiled_info, columns=['conversation_id', 'rating', 'date', 'conversation_length', 'user_text', 'system_text'])\n",
        "  return new_frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKTfuip-mMI6"
      },
      "source": [
        "data = []\n",
        "index = 0\n",
        "for logfile in all_files:\n",
        "  data.append(compile_texts(open_file(GROUP_FOLDER + \"/Processed Logs/\" + logfile), rating=index+1, min_length=5))\n",
        "  tokenize_frame_texts(data[-1])\n",
        "  index += 1\n",
        "data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxhQqllvHg_8"
      },
      "source": [
        "def gather_ngrams(dataframe_list, n):\n",
        "  ngramFDist = nltk.FreqDist()\n",
        "  for dataframe in dataframe_list:\n",
        "    for i in range(len(dataframe)):\n",
        "      tokens = [tok.lower() for tok in dataframe['user_tokens'][i]]\n",
        "      if len(tokens) >= n:\n",
        "        ngrams = nltk.ngrams(tokens, n=n)\n",
        "        for seq in ngrams:\n",
        "          ngramFDist[\" \".join(seq)] += 1\n",
        "  return ngramFDist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4Dhi6ijIji2"
      },
      "source": [
        "ngram_dist = gather_ngrams(data, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_VNOo1_u5lE"
      },
      "source": [
        "print(\"There were {} unique ngrams\".format(ngram_dist.B()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K33WIygGKe_y"
      },
      "source": [
        "ngram_dist.most_common(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Umb-veejEK_"
      },
      "source": [
        "ngram_dist.plot(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDY17NV9t5cJ"
      },
      "source": [
        "ngram_mean = np.mean(list(ngram_dist.values()))\n",
        "#ngram_std = np.std(list(ngram_dist.values()))\n",
        "values = list(ngram_dist.values())\n",
        "above_mean=0\n",
        "for i in range(len(values)):\n",
        "  if values[i] >= ngram_mean:\n",
        "    above_mean += 1\n",
        "  #print(\"Z-score: {}\".format(z_scores[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi63xDW6v4yx"
      },
      "source": [
        "#above_mean = np.sum(z_scores >= 0)\n",
        "print(\"{:.3f}% of the ngrams are above the mean. This amounts to {} unique ngrams.\".format(above_mean*100/ngram_dist.B(), above_mean))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M07HpUTkIUL"
      },
      "source": [
        "# Tensorflow Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz7KFa1ikLIw"
      },
      "source": [
        "# Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHehMMuw1smT"
      },
      "source": [
        "# Functions to plot loss and accuracy history\n",
        "def plot_losses(hist):\n",
        "    plt.plot(hist.history['loss'])\n",
        "    plt.plot(hist.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'])\n",
        "    plt.show()\n",
        "def plot_accuracies(hist):\n",
        "    plt.plot(hist.history['accuracy'])\n",
        "    plt.plot(hist.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7fBqcBimD4Z"
      },
      "source": [
        "# Get training frame (or use training_set from above)\n",
        "rating_1_training_raw = open_file(DATA_DIR+\"/Training Data/rating-1-training-set-profanity-tagged.tsv\")\n",
        "rating_1_compiled = compile_info(rating_1_training_raw, rating=1)\n",
        "rating_5_training_raw = open_file(DATA_DIR+\"/Training Data/rating-5-training-set-profanity-tagged.tsv\")\n",
        "rating_5_compiled = compile_info(rating_5_training_raw, rating=5)\n",
        "training_data = combine(rating_1_compiled, rating_5_compiled)\n",
        "#training_data = training_set\n",
        "training_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw1NZ52ylDYD"
      },
      "source": [
        "# Add column for class labels, label 0 means bad or rating 1, label 1 means good or rating 5\n",
        "training_data['class_label'] = training_data['rating'].apply(lambda val: 0 if val==1 else 1)\n",
        "training_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDUsr8ZCW81a"
      },
      "source": [
        "# Shuffle dataframe so the validation set isn't all one class\n",
        "training_data = training_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "training_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWeXnb9UskRg"
      },
      "source": [
        "# Get tuning frame\n",
        "rating_1_tuning_raw = open_file(DATA_DIR+\"/Tuning Data/rating-1-tuning-set-profanity-tagged.tsv\")\n",
        "rating_1_tuning_compiled = compile_info(rating_1_tuning_raw, rating=1)\n",
        "rating_5_tuning_raw = open_file(DATA_DIR+\"/Tuning Data/rating-5-tuning-set-profanity-tagged.tsv\")\n",
        "rating_5_tuning_compiled = compile_info(rating_5_tuning_raw, rating=5)\n",
        "tuning_data = combine(rating_1_tuning_compiled, rating_5_tuning_compiled)\n",
        "tuning_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sOcAC6UtD3V"
      },
      "source": [
        "# Add column for class labels, label 0 means bad or rating 1, label 1 means good or rating 5\n",
        "tuning_data['class_label'] = tuning_data['rating'].apply(lambda val: 0 if val==1 else 1)\n",
        "tuning_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFMY7x7Pr8k6"
      },
      "source": [
        "test = pd.DataFrame({'user_text':[training_data['user_text'][0]], 'system_text':[training_data['system_text'][0]]})\n",
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kScvzk0scsw"
      },
      "source": [
        "stitch_conversations_and_return(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAsvow1opPgp"
      },
      "source": [
        "# Instead of using just the user texts, let's stitch together the conversation turn by turn\n",
        "def stitch_conversations_and_return(dataframe):\n",
        "  conversations = []\n",
        "  for i in range(len(dataframe)):\n",
        "    stitch = \"\"\n",
        "    user_utters = dataframe['user_text'][i]\n",
        "    system_utters = dataframe['system_text'][i]\n",
        "    if len(user_utters)==len(system_utters):\n",
        "      for j in range(len(user_utters)):\n",
        "        stitch += user_utters[j] + \". \" + system_utters[j] + \". \"\n",
        "    elif len(user_utters) > len(system_utters):\n",
        "      for j in range(len(system_utters)):\n",
        "        stitch += user_utters[j] + \". \" + system_utters[j] + \". \"\n",
        "      for k in range(len(system_utters), len(user_utters), 1):\n",
        "        stitch += user_utters[k] + \". \"\n",
        "    else:\n",
        "      for j in range(len(user_utters)):\n",
        "        stitch += user_utters[j] + \". \" + system_utters[j] + \". \"\n",
        "      for k in range(len(user_utters), len(system_utters), 1):\n",
        "        stitch += user_utters[k] + \". \"\n",
        "\n",
        "    conversations.append(stitch)\n",
        "  return conversations\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y184AukEIMUP"
      },
      "source": [
        "# Set up text vectorization\n",
        "max_features = 20000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "# Get user texts\n",
        "#texts_ds = [\". \".join(text_list) for text_list in training_data['user_text']]\n",
        "texts_ds = stitch_conversations_and_return(training_data)\n",
        "print(texts_ds[:2])\n",
        "vectorize_layer.adapt(texts_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGUpo6f8qZNe"
      },
      "source": [
        "x_train = np.array(texts_ds)\n",
        "y_train = training_data['class_label'].to_numpy()\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_train[0], y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd2EygFRsbfy"
      },
      "source": [
        "#x_tuning = np.array([\". \".join(text_list) for text_list in tuning_data['user_text']])\n",
        "x_tuning = np.array(stitch_conversations_and_return(tuning_data))\n",
        "y_tuning = tuning_data['class_label'].to_numpy()\n",
        "print(x_tuning.shape, y_tuning.shape)\n",
        "print(x_tuning[0], y_tuning[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VL3OPiRJuRi"
      },
      "source": [
        "# Set up model structure to take raw strings, vectorize, embed, and set up output layers\n",
        "tf.keras.backend.clear_session()\n",
        "text_input = Input(shape=(1,), dtype=tf.string)\n",
        "x = vectorize_layer(text_input)\n",
        "x = Embedding(max_features, embedding_dim, mask_zero=True)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Conv1D(filters=128, kernel_size=7, strides=3, activation='relu')(x)\n",
        "x = Conv1D(filters=128, kernel_size=7, strides=3, activation='relu')(x)\n",
        "x = GlobalMaxPooling1D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=text_input, outputs=predictions)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygrHYqn_gNo9"
      },
      "source": [
        "# Set up another model structure, this time using an RNN\n",
        "tf.keras.backend.clear_session()\n",
        "text_input = Input(shape=(1,), dtype=tf.string)\n",
        "x = vectorize_layer(text_input)\n",
        "x = Embedding(max_features, embedding_dim, mask_zero=True)(x)\n",
        "x = Bidirectional(LSTM(units=64))(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=text_input, outputs=predictions)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRNb6aOBuQb_"
      },
      "source": [
        "CHECKPOINT_DIR = '/content/drive/MyDrive/Research/WalkerResearch/Model Checkpoints'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF_PQSNfc5Bh"
      },
      "source": [
        "print(\"Baseline Accuracy: {}\".format(np.sum(tuning_data['class_label'])/len(tuning_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbLLCTgVMO0k"
      },
      "source": [
        "# Compile and fit\n",
        "opt = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint_filepath = CHECKPOINT_DIR + \"/test3\"\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath = checkpoint_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only = True,\n",
        "    save_weights_only=True,\n",
        "    mode = 'max'\n",
        ")\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "          epochs=4,\n",
        "          verbose=1,\n",
        "          batch_size=128,\n",
        "          validation_split=0.1,\n",
        "          callbacks=[model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA3wXMoSuzc5"
      },
      "source": [
        "model.load_weights(checkpoint_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnGRu6kfrCjj"
      },
      "source": [
        "plot_accuracies(history)\n",
        "plot_losses(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcSAzxrTtj8D"
      },
      "source": [
        "# Test on the tuning set\n",
        "model.evaluate(x_tuning, y_tuning, batch_size=64, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhet8ISvQZ3z"
      },
      "source": [
        "# Get predictions\n",
        "pred_prob = model.predict(x_tuning)\n",
        "pred_class = []\n",
        "for i in range(len(pred_prob)):\n",
        "  if pred_prob[i] < 0.5:\n",
        "    pred_class.append(0)\n",
        "  else:\n",
        "    pred_class.append(1)\n",
        "pred_class = np.array(pred_class)\n",
        "print(pred_prob[0], pred_class[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24f3eTeCRET5"
      },
      "source": [
        "matrix = ConfusionMatrix(list(y_tuning), list(pred_class))\n",
        "TP = matrix[1, 1]\n",
        "TN = matrix[0, 0]\n",
        "FP = matrix[0, 1]\n",
        "FN = matrix[1, 0]\n",
        "acc = round((TP+TN)/(TP+TN+FP+FN), 3)\n",
        "prec = round(TP/(TP+FP), 3)\n",
        "recall = round(TP/(TP+FN), 3)\n",
        "f_score = round((2*prec*recall)/(prec + recall), 3)\n",
        "print(\"Accuracy: {}\".format(acc))\n",
        "print(\"Precision: {}\".format(prec))\n",
        "print(\"Recall: {}\".format(recall))\n",
        "print(\"f_measure: {}\".format(f_score))\n",
        "print(\"Confusion matrix:\\n{!s}\".format(matrix))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tByOQGYCG1Z-"
      },
      "source": [
        "# Automatic Evaluation\n",
        "Here, we use regression models to predict the overall rating of the conversations."
      ]
    }
  ]
}